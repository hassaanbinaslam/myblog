<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-10-18">
<meta name="description" content="This is a practice notebook for implementing a convolutional neural network (CNN) on the MNIST dataset with PyTorch. We will implement the now famous LeNet-5 from Yann LeCun, a 7-layer CNN from 1989. Then we will explore and visualize the layers learned by our network including filters, feature maps, and output layers.">

<title>Convolutional Neural Networks Filters and Feature Maps with PyTorch – Random Thoughts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-84543be43ff612bda7a31c913735130b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-D1ST9BH6HX"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D1ST9BH6HX', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Convolutional Neural Networks Filters and Feature Maps with PyTorch – Random Thoughts">
<meta property="og:description" content="This is a practice notebook for implementing a convolutional neural network (CNN) on the MNIST dataset with PyTorch. We will implement the now famous LeNet-5 from Yann LeCun, a 7-layer CNN from 1989. Then we will explore and visualize the layers learned by our network including filters, feature maps, and output layers.">
<meta property="og:image" content="images/2022-10-18-pytorch-mnist-convolutional-neural-networks.jpeg">
<meta property="og:site_name" content="Random Thoughts">
<meta name="twitter:title" content="Convolutional Neural Networks Filters and Feature Maps with PyTorch – Random Thoughts">
<meta name="twitter:description" content="This is a practice notebook for implementing a convolutional neural network (CNN) on the MNIST dataset with PyTorch. We will implement the now famous LeNet-5 from Yann LeCun, a 7-layer CNN from 1989. Then we will explore and visualize the layers learned by our network including filters, feature maps, and output layers.">
<meta name="twitter:image" content="images/2022-10-18-pytorch-mnist-convolutional-neural-networks.jpeg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Random Thoughts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hassaanbinaslam/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassaanbinaslam/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hassaanbinaslam"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#download-mnist-dataset" id="toc-download-mnist-dataset" class="nav-link" data-scroll-target="#download-mnist-dataset">Download MNIST Dataset</a></li>
  <li><a href="#load-generated-data-into-pytorch-dataset-and-dataloader-class" id="toc-load-generated-data-into-pytorch-dataset-and-dataloader-class" class="nav-link" data-scroll-target="#load-generated-data-into-pytorch-dataset-and-dataloader-class">Load generated data into PyTorch Dataset and DataLoader class</a></li>
  <li><a href="#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" id="toc-define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" class="nav-link" data-scroll-target="#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline">Define a class to implement training, validation, and mini-batch processing pipeline</a></li>
  <li><a href="#create-lenet-5-model-configuration" id="toc-create-lenet-5-model-configuration" class="nav-link" data-scroll-target="#create-lenet-5-model-configuration">Create LeNet-5 model configuration</a></li>
  <li><a href="#visualize-model-filters" id="toc-visualize-model-filters" class="nav-link" data-scroll-target="#visualize-model-filters">Visualize model filters</a>
  <ul class="collapse">
  <li><a href="#visualize-weights-for-first-conv2d-layer" id="toc-visualize-weights-for-first-conv2d-layer" class="nav-link" data-scroll-target="#visualize-weights-for-first-conv2d-layer">Visualize weights for first ‘Conv2d’ layer</a></li>
  <li><a href="#visualize-weights-for-second-conv2d-layer" id="toc-visualize-weights-for-second-conv2d-layer" class="nav-link" data-scroll-target="#visualize-weights-for-second-conv2d-layer">Visualize weights for second ‘Conv2d’ layer</a></li>
  <li><a href="#visualize-weights-for-third-conv2d-layer" id="toc-visualize-weights-for-third-conv2d-layer" class="nav-link" data-scroll-target="#visualize-weights-for-third-conv2d-layer">Visualize weights for third ‘Conv2d’ layer</a></li>
  <li><a href="#what-do-these-filter-images-tell-us" id="toc-what-do-these-filter-images-tell-us" class="nav-link" data-scroll-target="#what-do-these-filter-images-tell-us">What do these filter images tell us?</a></li>
  </ul></li>
  <li><a href="#visualize-feature-maps-for-convolutional-layers" id="toc-visualize-feature-maps-for-convolutional-layers" class="nav-link" data-scroll-target="#visualize-feature-maps-for-convolutional-layers">Visualize feature maps for convolutional layers</a>
  <ul class="collapse">
  <li><a href="#define-a-hook" id="toc-define-a-hook" class="nav-link" data-scroll-target="#define-a-hook">Define a hook</a></li>
  <li><a href="#feature-map-from-first-conv2d-layer" id="toc-feature-map-from-first-conv2d-layer" class="nav-link" data-scroll-target="#feature-map-from-first-conv2d-layer">Feature map from first ‘Conv2d’ layer</a></li>
  <li><a href="#feature-map-from-second-conv2d-layer" id="toc-feature-map-from-second-conv2d-layer" class="nav-link" data-scroll-target="#feature-map-from-second-conv2d-layer">Feature map from second ‘Conv2d’ layer</a></li>
  <li><a href="#feature-maps-from-the-first-and-second-conv2d-layers-together" id="toc-feature-maps-from-the-first-and-second-conv2d-layers-together" class="nav-link" data-scroll-target="#feature-maps-from-the-first-and-second-conv2d-layers-together">Feature maps from the first and second ‘Conv2d’ layers together</a></li>
  <li><a href="#feature-map-from-third-conv2d-layer" id="toc-feature-map-from-third-conv2d-layer" class="nav-link" data-scroll-target="#feature-map-from-third-conv2d-layer">Feature map from third ‘Conv2d’ layer</a></li>
  <li><a href="#feature-map-from-third-conv2d-layer-for-multiple-images" id="toc-feature-map-from-third-conv2d-layer-for-multiple-images" class="nav-link" data-scroll-target="#feature-map-from-third-conv2d-layer-for-multiple-images">Feature map from third ‘Conv2d’ layer for multiple images</a></li>
  </ul></li>
  <li><a href="#visualize-feature-maps-for-a-classifier-or-hidden-layer" id="toc-visualize-feature-maps-for-a-classifier-or-hidden-layer" class="nav-link" data-scroll-target="#visualize-feature-maps-for-a-classifier-or-hidden-layer">Visualize feature maps for a classifier or hidden layer</a>
  <ul class="collapse">
  <li><a href="#visualize-feature-map-for-first-linear-layer" id="toc-visualize-feature-map-for-first-linear-layer" class="nav-link" data-scroll-target="#visualize-feature-map-for-first-linear-layer">Visualize feature map for first ‘Linear’ layer</a></li>
  <li><a href="#visualize-feature-map-for-second-linear-layer" id="toc-visualize-feature-map-for-second-linear-layer" class="nav-link" data-scroll-target="#visualize-feature-map-for-second-linear-layer">Visualize feature map for second ‘Linear’ layer</a></li>
  <li><a href="#what-do-these-output-feature-maps-tell-us" id="toc-what-do-these-output-feature-maps-tell-us" class="nav-link" data-scroll-target="#what-do-these-output-feature-maps-tell-us">What do these OUTPUT feature maps tell us?</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Convolutional Neural Networks Filters and Feature Maps with PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
  </div>
  </div>

<div>
  <div class="description">
    This is a practice notebook for implementing a convolutional neural network (CNN) on the MNIST dataset with PyTorch. We will implement the now famous LeNet-5 from Yann LeCun, a 7-layer CNN from 1989. Then we will explore and visualize the layers learned by our network including filters, feature maps, and output layers.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 18, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="images/2022-10-18-pytorch-mnist-convolutional-neural-networks.jpeg" class="img-fluid"></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>We will train a LeNet-5 CNN model with PyTorch on the MNIST dataset in this notebook. Given below is the summary of the steps followed in this notebook.</p>
<ul>
<li>Download the MNIST dataset. Split the data into <code>Train</code> and <code>Validation</code> datasets. Then convert them into mini-batches using PyTorch <code>DataLoader</code> class</li>
<li>Create a Neural Net model configuration, an SGD optimizer, and a loss function</li>
<li>Create a pipeline that will train the model on given data and update the weights based on the loss</li>
<li>Visualize filters and feature maps from the trained model</li>
</ul>
</section>
<section id="environment" class="level2">
<h2 class="anchored" data-anchor-id="environment">Environment</h2>
<p>This notebook is prepared with Google Colab.</p>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/hassaanbinaslam/myblog/blob/main/posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.ipynb">2022-10-18-pytorch-mnist-convolutional-neural-networks.ipynb</a></li>
<li><strong>Open In Colab</strong>: <a href="https://colab.research.google.com/github/hassaanbinaslam/myblog/blob/main/posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Colab"></a></li>
</ul>
<div id="cell-4" class="cell" data-outputid="3623bbbf-f88d-4b90-d9d4-5c30df075659" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> platform <span class="im">import</span> python_version</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy, matplotlib, pandas, torch</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="bu">print</span>(<span class="st">"python=="</span> <span class="op">+</span> python_version())</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="bu">print</span>(<span class="st">"numpy=="</span> <span class="op">+</span> numpy.__version__)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="bu">print</span>(<span class="st">"torch=="</span> <span class="op">+</span> torch.__version__)</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="bu">print</span>(<span class="st">"matplotlib=="</span> <span class="op">+</span> matplotlib.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>python==3.7.15
numpy==1.21.6
torch==1.12.1+cu113
matplotlib==3.2.2</code></pre>
</div>
</div>
</section>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>This notebook takes inspiration and ideas from the following two sources.</p>
<ul>
<li>The outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: <a href="https://pytorchstepbystep.com/">pytorchstepbystep</a>. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: <a href="https://github.com/dvgodoy/PyTorchStepByStep">github.com/dvgodoy/PyTorchStepByStep</a>. Parts of the code you see in this notebook are taken from <a href="https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter05.ipynb">chapter 5 notebook</a> of the same book.</li>
<li>A great post by “Eugenia Anello” with the title <a href="https://medium.com/dataseries/visualizing-the-feature-maps-and-filters-by-convolutional-neural-networks-e1462340518e">Visualizing the Feature Maps and Filters by Convolutional Neural Networks</a>. You can click the title to reach the post on the <code>medium.com</code> platform. <code>Eugenia Anello</code> can also be found on her <a href="https://www.linkedin.com/in/eugenia-anello">linkedin profile</a>.</li>
</ul>
</section>
<section id="download-mnist-dataset" class="level2">
<h2 class="anchored" data-anchor-id="download-mnist-dataset">Download MNIST Dataset</h2>
<p>MNIST dataset can be downloaded easily from PyTorch built-in datasets provided under <code>torchvision.datasets</code>. In this section, we will download it, split it into train and test datasets, and then convert it into PyTorch tensors.</p>
<ul>
<li>Read more about the PyTorch MNIST dataset <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST">here</a></li>
<li><code>torchvision.transforms.Compose</code> is like a container to hold a list of transformations you intend to apply. Read more about it <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html">here</a></li>
<li><code>torchvision.transforms.ToTensor</code> converts a <code>PIL Image</code> or <code>numpy.ndarray</code> to tensor. It converts a PIL Image or numpy.ndarray <strong>(H x W x C) in the range [0, 255]</strong> to a torch.FloatTensor of shape <strong>(C x H x W) in the range [0.0, 1.0]</strong>. Here C=Channel, H=Height, W=Width. Read more about this transformation <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html">here</a></li>
</ul>
<div id="cell-7" class="cell" data-outputid="b05959d6-bffa-4084-9125-42279f9d9fcf" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co">#collapse-output</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> torchvision</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>train_dataset <span class="op">=</span> torchvision.datasets.MNIST(<span class="st">'classifier_data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>test_dataset  <span class="op">=</span> torchvision.datasets.MNIST(<span class="st">'classifier_data'</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a>transform <span class="op">=</span> torchvision.transforms.Compose([</span>
<span id="cb3-9"><a href="#cb3-9"></a>    torchvision.transforms.ToTensor()</span>
<span id="cb3-10"><a href="#cb3-10"></a>])</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a>train_dataset.transform<span class="op">=</span>transform</span>
<span id="cb3-13"><a href="#cb3-13"></a>test_dataset.transform<span class="op">=</span>transform</span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="bu">print</span>(<span class="ss">f"Total training images: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-16"><a href="#cb3-16"></a><span class="bu">print</span>(<span class="ss">f"Shape of an image: </span><span class="sc">{</span>np<span class="sc">.</span>shape(train_dataset.data[<span class="dv">7</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="bu">print</span>(<span class="ss">f"Values of an image: </span><span class="ch">\n</span><span class="sc">{</span>train_dataset<span class="sc">.</span>data[<span class="dv">7</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total training images: 60000
Shape of an image: torch.Size([28, 28])
Values of an image: 
tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105,
         255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,
         253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252, 252,
         253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230, 132,
         133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,   0,
           0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193, 193,
         253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252, 252,
         253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253, 253,
         255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,  44,
          44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,
           0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,  18,
          92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134, 203,
         253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252, 252,
         253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217, 207,
         146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],
       dtype=torch.uint8)</code></pre>
</div>
</div>
<p>From the above cell output, there are <code>60,000</code> training images. The shape of each image is <code>28 x 28</code>, which means it is a 2D matrix. We have also printed the values of one image, but they don’t make much sense unless we view them as an image. So let’s do that.</p>
<div id="cell-9" class="cell" data-outputid="88790e7b-a6a1-426f-c20f-d6550b66c1be" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a>plt.imshow(train_dataset.data[<span class="dv">7</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="load-generated-data-into-pytorch-dataset-and-dataloader-class" class="level2">
<h2 class="anchored" data-anchor-id="load-generated-data-into-pytorch-dataset-and-dataloader-class">Load generated data into PyTorch Dataset and DataLoader class</h2>
<p>Now let’s load our data into <code>Dataset</code> and <code>DataLoader</code> classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. <code>batch_size</code> means the number of tuples we want in a single batch. We have used 128 here, so each fetch from DataLoader will give us a list of 128 tuples.</p>
<div id="cell-11" class="cell" data-outputid="f198446f-7980-4a5f-da67-6f57b5fb5212" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, random_split</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>train_size<span class="op">=</span><span class="bu">len</span>(train_dataset)</span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co"># Randomly split the data into non-overlapping train and validation set</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co"># train size = 70% and validation size = 30%</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>train_data, val_data <span class="op">=</span> random_split(train_dataset, [<span class="bu">int</span>(train_size<span class="op">*</span><span class="fl">0.7</span>), <span class="bu">int</span>(train_size <span class="op">-</span> train_size<span class="op">*</span><span class="fl">0.7</span>)])</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a>batch_size<span class="op">=</span><span class="dv">128</span></span>
<span id="cb6-11"><a href="#cb6-11"></a></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="co"># Load data into DataLoader class</span></span>
<span id="cb6-13"><a href="#cb6-13"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb6-14"><a href="#cb6-14"></a>valid_loader <span class="op">=</span> torch.utils.data.DataLoader(val_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="bu">print</span>(<span class="ss">f"Batches in Train Loader: </span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="bu">print</span>(<span class="ss">f"Batches in Valid Loader: </span><span class="sc">{</span><span class="bu">len</span>(valid_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a><span class="bu">print</span>(<span class="ss">f"Examples in Train Loader: </span><span class="sc">{</span><span class="bu">len</span>(train_loader.sampler)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-20"><a href="#cb6-20"></a><span class="bu">print</span>(<span class="ss">f"Examples in Valid Loader: </span><span class="sc">{</span><span class="bu">len</span>(valid_loader.sampler)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batches in Train Loader: 469
Batches in Valid Loader: 141
Examples in Train Loader: 60000
Examples in Valid Loader: 18000</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co">##</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># Helper function to plot images from DataLoader</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="kw">def</span> plot_images(images, targets, n_plot<span class="op">=</span><span class="dv">30</span>):</span>
<span id="cb8-4"><a href="#cb8-4"></a>    n_rows <span class="op">=</span> n_plot <span class="op">//</span> <span class="dv">10</span> <span class="op">+</span> ((n_plot <span class="op">%</span> <span class="dv">10</span>) <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb8-5"><a href="#cb8-5"></a>    fig, axes <span class="op">=</span> plt.subplots(n_rows, <span class="dv">10</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="fl">1.5</span> <span class="op">*</span> n_rows))</span>
<span id="cb8-6"><a href="#cb8-6"></a>    axes <span class="op">=</span> np.atleast_2d(axes)</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a>    <span class="cf">for</span> i, (image, target) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(images[:n_plot], targets[:n_plot])):</span>
<span id="cb8-9"><a href="#cb8-9"></a>        row, col <span class="op">=</span> i <span class="op">//</span> <span class="dv">10</span>, i <span class="op">%</span> <span class="dv">10</span>    </span>
<span id="cb8-10"><a href="#cb8-10"></a>        ax <span class="op">=</span> axes[row, col]</span>
<span id="cb8-11"><a href="#cb8-11"></a>        ax.set_title(<span class="st">'#</span><span class="sc">{}</span><span class="st"> - Label:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i, target), {<span class="st">'size'</span>: <span class="dv">12</span>})</span>
<span id="cb8-12"><a href="#cb8-12"></a>        <span class="co"># plot filter channel in grayscale</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>        ax.imshow(image.squeeze(), cmap<span class="op">=</span><span class="st">'gray'</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a>    <span class="cf">for</span> ax <span class="kw">in</span> axes.flat:</span>
<span id="cb8-16"><a href="#cb8-16"></a>        ax.set_xticks([])</span>
<span id="cb8-17"><a href="#cb8-17"></a>        ax.set_yticks([])</span>
<span id="cb8-18"><a href="#cb8-18"></a>        ax.label_outer()</span>
<span id="cb8-19"><a href="#cb8-19"></a></span>
<span id="cb8-20"><a href="#cb8-20"></a>    plt.tight_layout()</span>
<span id="cb8-21"><a href="#cb8-21"></a>    <span class="cf">return</span> fig</span>
<span id="cb8-22"><a href="#cb8-22"></a></span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="co">## Code taken from https://github.com/dvgodoy/PyTorchStepByStep/blob/master/plots/chapter5.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s plot some dataset images along with their labels from a batch.</p>
<div id="cell-14" class="cell" data-outputid="26584fa3-41ff-4c8e-e7ed-bf2470b688bf" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb9-2"><a href="#cb9-2"></a>fig <span class="op">=</span> plot_images(images, labels, n_plot<span class="op">=</span><span class="dv">40</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline">Define a class to implement training, validation, and mini-batch processing pipeline</h2>
<p>In this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are</p>
<ul>
<li><code>__init__</code>: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders</li>
<li><code>_make_train_step_fn</code>: Training pipeline is usually called “training step” which includes the following steps
<ol type="1">
<li>Compute our model’s predicted output - the forward pass</li>
<li>Compute the loss</li>
<li>Compute gradients i.e., find the direction and scale to update the weights to reduce the loss</li>
<li>Update weight parameters using gradients and the learning rate</li>
</ol></li>
<li><code>_make_val_step_fn</code>: Validation pipeline is usually called the “validation step” which includes the following steps
<ol type="1">
<li>Compute our model’s predicted output - the forward pass</li>
<li>Compute the loss</li>
<li>Note that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.</li>
</ol></li>
<li><code>_mini_batch</code>: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to
<ol type="1">
<li>Get the next batch of data and labels (x, y) from the DataLoader iterator</li>
<li>Perform a step on the batch. A step can be either training or validation</li>
<li>Compute the average batch loss</li>
</ol></li>
<li><code>train</code>: Execute training and validation steps for given number of epoch</li>
<li><code>predict</code>: Make a prediction from model on provided data</li>
</ul>
<div id="cell-16" class="cell">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">class</span> DeepLearningPipeline(<span class="bu">object</span>):</span>
<span id="cb10-2"><a href="#cb10-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, loss_fn, optimizer):</span>
<span id="cb10-3"><a href="#cb10-3"></a>        <span class="co"># Here we define the attributes of our class</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>        </span>
<span id="cb10-5"><a href="#cb10-5"></a>        <span class="co"># We start by storing the arguments as attributes </span></span>
<span id="cb10-6"><a href="#cb10-6"></a>        <span class="co"># to use them later</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb10-8"><a href="#cb10-8"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> loss_fn</span>
<span id="cb10-9"><a href="#cb10-9"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> optimizer</span>
<span id="cb10-10"><a href="#cb10-10"></a>        <span class="va">self</span>.device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="co"># Let's send the model to the specified device right away</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>        <span class="va">self</span>.model.to(<span class="va">self</span>.device)</span>
<span id="cb10-13"><a href="#cb10-13"></a></span>
<span id="cb10-14"><a href="#cb10-14"></a>        <span class="co"># These attributes are defined here, but since they are</span></span>
<span id="cb10-15"><a href="#cb10-15"></a>        <span class="co"># not informed at the moment of creation, we keep them None</span></span>
<span id="cb10-16"><a href="#cb10-16"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-17"><a href="#cb10-17"></a>        <span class="va">self</span>.val_loader <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-18"><a href="#cb10-18"></a>        <span class="va">self</span>.writer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-19"><a href="#cb10-19"></a>        </span>
<span id="cb10-20"><a href="#cb10-20"></a>        <span class="co"># These attributes are going to be computed internally</span></span>
<span id="cb10-21"><a href="#cb10-21"></a>        <span class="va">self</span>.losses <span class="op">=</span> []</span>
<span id="cb10-22"><a href="#cb10-22"></a>        <span class="va">self</span>.val_losses <span class="op">=</span> []</span>
<span id="cb10-23"><a href="#cb10-23"></a>        <span class="va">self</span>.total_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-24"><a href="#cb10-24"></a></span>
<span id="cb10-25"><a href="#cb10-25"></a>        <span class="co"># Creates the train_step function for our model, </span></span>
<span id="cb10-26"><a href="#cb10-26"></a>        <span class="co"># loss function and optimizer</span></span>
<span id="cb10-27"><a href="#cb10-27"></a>        <span class="co"># Note: there are NO ARGS there! It makes use of the class</span></span>
<span id="cb10-28"><a href="#cb10-28"></a>        <span class="co"># attributes directly</span></span>
<span id="cb10-29"><a href="#cb10-29"></a>        <span class="va">self</span>.train_step_fn <span class="op">=</span> <span class="va">self</span>._make_train_step_fn()</span>
<span id="cb10-30"><a href="#cb10-30"></a>        <span class="co"># Creates the val_step function for our model and loss</span></span>
<span id="cb10-31"><a href="#cb10-31"></a>        <span class="va">self</span>.val_step_fn <span class="op">=</span> <span class="va">self</span>._make_val_step_fn()</span>
<span id="cb10-32"><a href="#cb10-32"></a></span>
<span id="cb10-33"><a href="#cb10-33"></a>    <span class="kw">def</span> set_loaders(<span class="va">self</span>, train_loader, val_loader<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-34"><a href="#cb10-34"></a>        <span class="co"># This method allows the user to define which train_loader (and val_loader, optionally) to use</span></span>
<span id="cb10-35"><a href="#cb10-35"></a>        <span class="co"># Both loaders are then assigned to attributes of the class</span></span>
<span id="cb10-36"><a href="#cb10-36"></a>        <span class="co"># So they can be referred to later</span></span>
<span id="cb10-37"><a href="#cb10-37"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> train_loader</span>
<span id="cb10-38"><a href="#cb10-38"></a>        <span class="va">self</span>.val_loader <span class="op">=</span> val_loader</span>
<span id="cb10-39"><a href="#cb10-39"></a></span>
<span id="cb10-40"><a href="#cb10-40"></a>    <span class="kw">def</span> _make_train_step_fn(<span class="va">self</span>):</span>
<span id="cb10-41"><a href="#cb10-41"></a>        <span class="co"># This method does not need ARGS... it can refer to</span></span>
<span id="cb10-42"><a href="#cb10-42"></a>        <span class="co"># the attributes: self.model, self.loss_fn and self.optimizer</span></span>
<span id="cb10-43"><a href="#cb10-43"></a>        </span>
<span id="cb10-44"><a href="#cb10-44"></a>        <span class="co"># Builds function that performs a step in the train loop</span></span>
<span id="cb10-45"><a href="#cb10-45"></a>        <span class="kw">def</span> perform_train_step_fn(x, y):</span>
<span id="cb10-46"><a href="#cb10-46"></a>            <span class="co"># Sets model to TRAIN mode</span></span>
<span id="cb10-47"><a href="#cb10-47"></a>            <span class="va">self</span>.model.train()</span>
<span id="cb10-48"><a href="#cb10-48"></a></span>
<span id="cb10-49"><a href="#cb10-49"></a>            <span class="co"># Step 1 - Computes our model's predicted output - forward pass</span></span>
<span id="cb10-50"><a href="#cb10-50"></a>            yhat <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb10-51"><a href="#cb10-51"></a>            <span class="co"># Step 2 - Computes the loss</span></span>
<span id="cb10-52"><a href="#cb10-52"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_fn(yhat, y)</span>
<span id="cb10-53"><a href="#cb10-53"></a>            <span class="co"># Step 3 - Computes gradients for both "a" and "b" parameters</span></span>
<span id="cb10-54"><a href="#cb10-54"></a>            loss.backward()</span>
<span id="cb10-55"><a href="#cb10-55"></a>            <span class="co"># Step 4 - Updates parameters using gradients and the learning rate</span></span>
<span id="cb10-56"><a href="#cb10-56"></a>            <span class="va">self</span>.optimizer.step()</span>
<span id="cb10-57"><a href="#cb10-57"></a>            <span class="va">self</span>.optimizer.zero_grad()</span>
<span id="cb10-58"><a href="#cb10-58"></a></span>
<span id="cb10-59"><a href="#cb10-59"></a>            <span class="co"># Returns the loss</span></span>
<span id="cb10-60"><a href="#cb10-60"></a>            <span class="cf">return</span> loss.item()</span>
<span id="cb10-61"><a href="#cb10-61"></a></span>
<span id="cb10-62"><a href="#cb10-62"></a>        <span class="co"># Returns the function that will be called inside the train loop</span></span>
<span id="cb10-63"><a href="#cb10-63"></a>        <span class="cf">return</span> perform_train_step_fn</span>
<span id="cb10-64"><a href="#cb10-64"></a>    </span>
<span id="cb10-65"><a href="#cb10-65"></a>    <span class="kw">def</span> _make_val_step_fn(<span class="va">self</span>):</span>
<span id="cb10-66"><a href="#cb10-66"></a>        <span class="co"># Builds function that performs a step in the validation loop</span></span>
<span id="cb10-67"><a href="#cb10-67"></a>        <span class="kw">def</span> perform_val_step_fn(x, y):</span>
<span id="cb10-68"><a href="#cb10-68"></a>            <span class="co"># Sets model to EVAL mode</span></span>
<span id="cb10-69"><a href="#cb10-69"></a>            <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb10-70"><a href="#cb10-70"></a></span>
<span id="cb10-71"><a href="#cb10-71"></a>            <span class="co"># Step 1 - Computes our model's predicted output - forward pass</span></span>
<span id="cb10-72"><a href="#cb10-72"></a>            yhat <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb10-73"><a href="#cb10-73"></a>            <span class="co"># Step 2 - Computes the loss</span></span>
<span id="cb10-74"><a href="#cb10-74"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_fn(yhat, y)</span>
<span id="cb10-75"><a href="#cb10-75"></a>            <span class="co"># There is no need to compute Steps 3 and 4, </span></span>
<span id="cb10-76"><a href="#cb10-76"></a>            <span class="co"># since we don't update parameters during evaluation</span></span>
<span id="cb10-77"><a href="#cb10-77"></a>            <span class="cf">return</span> loss.item()</span>
<span id="cb10-78"><a href="#cb10-78"></a></span>
<span id="cb10-79"><a href="#cb10-79"></a>        <span class="cf">return</span> perform_val_step_fn</span>
<span id="cb10-80"><a href="#cb10-80"></a>            </span>
<span id="cb10-81"><a href="#cb10-81"></a>    <span class="kw">def</span> _mini_batch(<span class="va">self</span>, validation<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb10-82"><a href="#cb10-82"></a>        <span class="co"># The mini-batch can be used with both loaders</span></span>
<span id="cb10-83"><a href="#cb10-83"></a>        <span class="co"># The argument `validation`defines which loader and </span></span>
<span id="cb10-84"><a href="#cb10-84"></a>        <span class="co"># corresponding step function is going to be used</span></span>
<span id="cb10-85"><a href="#cb10-85"></a>        <span class="cf">if</span> validation:</span>
<span id="cb10-86"><a href="#cb10-86"></a>            data_loader <span class="op">=</span> <span class="va">self</span>.val_loader</span>
<span id="cb10-87"><a href="#cb10-87"></a>            step_fn <span class="op">=</span> <span class="va">self</span>.val_step_fn</span>
<span id="cb10-88"><a href="#cb10-88"></a>        <span class="cf">else</span>:</span>
<span id="cb10-89"><a href="#cb10-89"></a>            data_loader <span class="op">=</span> <span class="va">self</span>.train_loader</span>
<span id="cb10-90"><a href="#cb10-90"></a>            step_fn <span class="op">=</span> <span class="va">self</span>.train_step_fn</span>
<span id="cb10-91"><a href="#cb10-91"></a></span>
<span id="cb10-92"><a href="#cb10-92"></a>        <span class="cf">if</span> data_loader <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb10-93"><a href="#cb10-93"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb10-94"><a href="#cb10-94"></a>            </span>
<span id="cb10-95"><a href="#cb10-95"></a>        <span class="co"># Once the data loader and step function, this is the </span></span>
<span id="cb10-96"><a href="#cb10-96"></a>        <span class="co"># same mini-batch loop we had before</span></span>
<span id="cb10-97"><a href="#cb10-97"></a>        mini_batch_losses <span class="op">=</span> []</span>
<span id="cb10-98"><a href="#cb10-98"></a>        <span class="cf">for</span> x_batch, y_batch <span class="kw">in</span> data_loader:</span>
<span id="cb10-99"><a href="#cb10-99"></a>            x_batch <span class="op">=</span> x_batch.to(<span class="va">self</span>.device)</span>
<span id="cb10-100"><a href="#cb10-100"></a>            y_batch <span class="op">=</span> y_batch.to(<span class="va">self</span>.device)</span>
<span id="cb10-101"><a href="#cb10-101"></a></span>
<span id="cb10-102"><a href="#cb10-102"></a>            mini_batch_loss <span class="op">=</span> step_fn(x_batch, y_batch)</span>
<span id="cb10-103"><a href="#cb10-103"></a>            mini_batch_losses.append(mini_batch_loss)</span>
<span id="cb10-104"><a href="#cb10-104"></a></span>
<span id="cb10-105"><a href="#cb10-105"></a>        loss <span class="op">=</span> np.mean(mini_batch_losses)</span>
<span id="cb10-106"><a href="#cb10-106"></a>        <span class="cf">return</span> loss</span>
<span id="cb10-107"><a href="#cb10-107"></a></span>
<span id="cb10-108"><a href="#cb10-108"></a>    <span class="kw">def</span> set_seed(<span class="va">self</span>, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb10-109"><a href="#cb10-109"></a>        torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-110"><a href="#cb10-110"></a>        torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span>    </span>
<span id="cb10-111"><a href="#cb10-111"></a>        torch.manual_seed(seed)</span>
<span id="cb10-112"><a href="#cb10-112"></a>        np.random.seed(seed)</span>
<span id="cb10-113"><a href="#cb10-113"></a>    </span>
<span id="cb10-114"><a href="#cb10-114"></a>    <span class="kw">def</span> train(<span class="va">self</span>, n_epochs, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb10-115"><a href="#cb10-115"></a>        <span class="co"># To ensure reproducibility of the training process</span></span>
<span id="cb10-116"><a href="#cb10-116"></a>        <span class="va">self</span>.set_seed(seed)</span>
<span id="cb10-117"><a href="#cb10-117"></a></span>
<span id="cb10-118"><a href="#cb10-118"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb10-119"><a href="#cb10-119"></a>            <span class="co"># Keeps track of the numbers of epochs</span></span>
<span id="cb10-120"><a href="#cb10-120"></a>            <span class="co"># by updating the corresponding attribute</span></span>
<span id="cb10-121"><a href="#cb10-121"></a>            <span class="va">self</span>.total_epochs <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-122"><a href="#cb10-122"></a></span>
<span id="cb10-123"><a href="#cb10-123"></a>            <span class="co"># inner loop</span></span>
<span id="cb10-124"><a href="#cb10-124"></a>            <span class="co"># Performs training using mini-batches</span></span>
<span id="cb10-125"><a href="#cb10-125"></a>            loss <span class="op">=</span> <span class="va">self</span>._mini_batch(validation<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-126"><a href="#cb10-126"></a>            <span class="va">self</span>.losses.append(loss)</span>
<span id="cb10-127"><a href="#cb10-127"></a></span>
<span id="cb10-128"><a href="#cb10-128"></a>            <span class="co"># VALIDATION</span></span>
<span id="cb10-129"><a href="#cb10-129"></a>            <span class="co"># no gradients in validation!</span></span>
<span id="cb10-130"><a href="#cb10-130"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-131"><a href="#cb10-131"></a>                <span class="co"># Performs evaluation using mini-batches</span></span>
<span id="cb10-132"><a href="#cb10-132"></a>                val_loss <span class="op">=</span> <span class="va">self</span>._mini_batch(validation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-133"><a href="#cb10-133"></a>                <span class="va">self</span>.val_losses.append(val_loss)</span>
<span id="cb10-134"><a href="#cb10-134"></a></span>
<span id="cb10-135"><a href="#cb10-135"></a>            <span class="co"># If a SummaryWriter has been set...</span></span>
<span id="cb10-136"><a href="#cb10-136"></a>            <span class="cf">if</span> <span class="va">self</span>.writer:</span>
<span id="cb10-137"><a href="#cb10-137"></a>                scalars <span class="op">=</span> {<span class="st">'training'</span>: loss}</span>
<span id="cb10-138"><a href="#cb10-138"></a>                <span class="cf">if</span> val_loss <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-139"><a href="#cb10-139"></a>                    scalars.update({<span class="st">'validation'</span>: val_loss})</span>
<span id="cb10-140"><a href="#cb10-140"></a>                <span class="co"># Records both losses for each epoch under the main tag "loss"</span></span>
<span id="cb10-141"><a href="#cb10-141"></a>                <span class="va">self</span>.writer.add_scalars(main_tag<span class="op">=</span><span class="st">'loss'</span>,</span>
<span id="cb10-142"><a href="#cb10-142"></a>                                        tag_scalar_dict<span class="op">=</span>scalars,</span>
<span id="cb10-143"><a href="#cb10-143"></a>                                        global_step<span class="op">=</span>epoch)</span>
<span id="cb10-144"><a href="#cb10-144"></a>            </span>
<span id="cb10-145"><a href="#cb10-145"></a>            <span class="bu">print</span>(<span class="ss">f"epoch: </span><span class="sc">{</span>epoch<span class="sc">:3}</span><span class="ss">, train loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, valid loss: </span><span class="sc">{</span>val_loss<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb10-146"><a href="#cb10-146"></a></span>
<span id="cb10-147"><a href="#cb10-147"></a>        <span class="cf">if</span> <span class="va">self</span>.writer:</span>
<span id="cb10-148"><a href="#cb10-148"></a>            <span class="co"># Closes the writer</span></span>
<span id="cb10-149"><a href="#cb10-149"></a>            <span class="va">self</span>.writer.close()</span>
<span id="cb10-150"><a href="#cb10-150"></a></span>
<span id="cb10-151"><a href="#cb10-151"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb10-152"><a href="#cb10-152"></a>        <span class="co"># Set is to evaluation mode for predictions</span></span>
<span id="cb10-153"><a href="#cb10-153"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>() </span>
<span id="cb10-154"><a href="#cb10-154"></a>        <span class="co"># Takes aNumpy input and make it a float tensor</span></span>
<span id="cb10-155"><a href="#cb10-155"></a>        x_tensor <span class="op">=</span> torch.as_tensor(x).<span class="bu">float</span>()</span>
<span id="cb10-156"><a href="#cb10-156"></a>        <span class="co"># Send input to device and uses model for prediction</span></span>
<span id="cb10-157"><a href="#cb10-157"></a>        y_hat_tensor <span class="op">=</span> <span class="va">self</span>.model(x_tensor.to(<span class="va">self</span>.device))</span>
<span id="cb10-158"><a href="#cb10-158"></a>        <span class="co"># Set it back to train mode</span></span>
<span id="cb10-159"><a href="#cb10-159"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb10-160"><a href="#cb10-160"></a>        <span class="co"># Detaches it, brings it to CPU and back to Numpy</span></span>
<span id="cb10-161"><a href="#cb10-161"></a>        <span class="cf">return</span> y_hat_tensor.detach().cpu().numpy()</span>
<span id="cb10-162"><a href="#cb10-162"></a></span>
<span id="cb10-163"><a href="#cb10-163"></a>    <span class="kw">def</span> plot_losses(<span class="va">self</span>):</span>
<span id="cb10-164"><a href="#cb10-164"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb10-165"><a href="#cb10-165"></a>        plt.plot(<span class="va">self</span>.losses, label<span class="op">=</span><span class="st">'Training Loss'</span>, c<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb10-166"><a href="#cb10-166"></a>        plt.plot(<span class="va">self</span>.val_losses, label<span class="op">=</span><span class="st">'Validation Loss'</span>, c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb10-167"><a href="#cb10-167"></a>        plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb10-168"><a href="#cb10-168"></a>        plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb10-169"><a href="#cb10-169"></a>        plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb10-170"><a href="#cb10-170"></a>        plt.legend()</span>
<span id="cb10-171"><a href="#cb10-171"></a>        plt.tight_layout()</span>
<span id="cb10-172"><a href="#cb10-172"></a>        <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="create-lenet-5-model-configuration" class="level2">
<h2 class="anchored" data-anchor-id="create-lenet-5-model-configuration">Create LeNet-5 model configuration</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2022-10-18-pytorch-mnist-convolutional-neural-networks/architecture_lenet.png" class="img-fluid figure-img"></p>
<figcaption>architecture_lenet.png</figcaption>
</figure>
</div>
<p><em>Source: Generated using Alexander Lenail’s <a href="http://alexlenail.me/NN-SVG/">NN-SVG</a> and adapted by the author [“Daniel Voigt Godoy”]. For more details, see LeCun, Y., et al (1998). <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-based learning applied to document recognition</a>. Proceedings of the IEEE,86(11), 2278–2324</em></p>
<p><em>Image taken from <a href="https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/images/architecture_lenet.png">dvgodoy/PyTorchStepByStep</a></em></p>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>lenet <span class="op">=</span> nn.Sequential()</span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="co"># Featurizer</span></span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="co"># Block 1: 1@28x28 -&gt; 6@28x28 -&gt; 6@14x14</span></span>
<span id="cb11-7"><a href="#cb11-7"></a>lenet.add_module(<span class="st">'C1'</span>, nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">6</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb11-8"><a href="#cb11-8"></a>lenet.add_module(<span class="st">'func1'</span>, nn.ReLU())</span>
<span id="cb11-9"><a href="#cb11-9"></a>lenet.add_module(<span class="st">'S2'</span>, nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="co"># Block 2: 6@14x14 -&gt; 16@10x10 -&gt; 16@5x5</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>lenet.add_module(<span class="st">'C3'</span>, nn.Conv2d(in_channels<span class="op">=</span><span class="dv">6</span>, out_channels<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb11-12"><a href="#cb11-12"></a>lenet.add_module(<span class="st">'func2'</span>, nn.ReLU())</span>
<span id="cb11-13"><a href="#cb11-13"></a>lenet.add_module(<span class="st">'S4'</span>, nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="co"># Block 3: 16@5x5 -&gt; 120@1x1</span></span>
<span id="cb11-15"><a href="#cb11-15"></a>lenet.add_module(<span class="st">'C5'</span>, nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">120</span>, kernel_size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb11-16"><a href="#cb11-16"></a>lenet.add_module(<span class="st">'func2'</span>, nn.ReLU())</span>
<span id="cb11-17"><a href="#cb11-17"></a><span class="co"># Flattening</span></span>
<span id="cb11-18"><a href="#cb11-18"></a>lenet.add_module(<span class="st">'flatten'</span>, nn.Flatten())</span>
<span id="cb11-19"><a href="#cb11-19"></a></span>
<span id="cb11-20"><a href="#cb11-20"></a><span class="co"># Classification</span></span>
<span id="cb11-21"><a href="#cb11-21"></a><span class="co"># Hidden Layer</span></span>
<span id="cb11-22"><a href="#cb11-22"></a>lenet.add_module(<span class="st">'F6'</span>, nn.Linear(in_features<span class="op">=</span><span class="dv">120</span>, out_features<span class="op">=</span><span class="dv">84</span>))</span>
<span id="cb11-23"><a href="#cb11-23"></a>lenet.add_module(<span class="st">'func3'</span>, nn.ReLU())</span>
<span id="cb11-24"><a href="#cb11-24"></a><span class="co"># Output Layer</span></span>
<span id="cb11-25"><a href="#cb11-25"></a>lenet.add_module(<span class="st">'OUTPUT'</span>, nn.Linear(in_features<span class="op">=</span><span class="dv">84</span>, out_features<span class="op">=</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s create optimizer and a loss function.</p>
<div id="cell-20" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a>lr <span class="op">=</span> <span class="fl">0.003</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb12-5"><a href="#cb12-5"></a></span>
<span id="cb12-6"><a href="#cb12-6"></a>model <span class="op">=</span> lenet</span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="co"># Defines a SGD optimizer to update the parameters</span></span>
<span id="cb12-9"><a href="#cb12-9"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb12-10"><a href="#cb12-10"></a></span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="co"># Defines a BCE loss function</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s train our model for 20 epochs.</p>
<div id="cell-22" class="cell" data-outputid="8348cf19-f0b9-42a5-a1cc-95343ac26af6">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>n_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>dlp <span class="op">=</span> DeepLearningPipeline(model, loss_fn, optimizer)</span>
<span id="cb13-4"><a href="#cb13-4"></a>dlp.set_loaders(train_loader, valid_loader)</span>
<span id="cb13-5"><a href="#cb13-5"></a>dlp.train(n_epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch:   0, train loss: 2.29349, valid loss: 2.28632
epoch:   1, train loss: 2.27442, valid loss: 2.25871
epoch:   2, train loss: 2.22077, valid loss: 2.15503
epoch:   3, train loss: 1.85172, valid loss: 1.27419
epoch:   4, train loss: 0.82061, valid loss: 0.59110
epoch:   5, train loss: 0.50765, valid loss: 0.45872
epoch:   6, train loss: 0.41486, valid loss: 0.39295
epoch:   7, train loss: 0.36466, valid loss: 0.35130
epoch:   8, train loss: 0.33108, valid loss: 0.32108
epoch:   9, train loss: 0.30512, valid loss: 0.29665
epoch:  10, train loss: 0.28347, valid loss: 0.27550
epoch:  11, train loss: 0.26473, valid loss: 0.25711
epoch:  12, train loss: 0.24807, valid loss: 0.24056
epoch:  13, train loss: 0.23314, valid loss: 0.22590
epoch:  14, train loss: 0.21989, valid loss: 0.21282
epoch:  15, train loss: 0.20804, valid loss: 0.20097
epoch:  16, train loss: 0.19743, valid loss: 0.19028
epoch:  17, train loss: 0.18791, valid loss: 0.18065
epoch:  18, train loss: 0.17929, valid loss: 0.17199
epoch:  19, train loss: 0.17142, valid loss: 0.16408</code></pre>
</div>
</div>
<p>Let’s see how our training and validation loss looks like.</p>
<div id="cell-24" class="cell" data-outputid="7d1cca70-43b6-43b4-ffe7-0818bd8d32b1">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>fig <span class="op">=</span> dlp.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualize-model-filters" class="level2">
<h2 class="anchored" data-anchor-id="visualize-model-filters">Visualize model filters</h2>
<p>Neural network convolutional layers are a stack of square matrices. We repeatedly apply these matrices or <code>filters</code> on images, and the output of this operation is called <code>convolutions</code>. These convolutions act as intermediate (or new) datasets generated from the images (a kind of dynamic feature engineering). We try to learn from these convolutions and calculate a loss for them. Initially, the value for these filters is randomly selected. If the loss is high, we try slowly changing values or <code>weights</code> for these filters. Changing filter values also create changes in outputs or convolutions. If a convolution results in a slight decrease in loss, we take it as a good sign and try to move in that direction (make more similar changes). If we try these steps repeatedly, we might find good convolutions (or good weights in the filter, as they are the source of convolutions). By “good weights”, we mean that the final loss is significantly lower than a random value.</p>
<p>These convolutional filters are created in PyTorch using class <code>nn.Conv2d</code>. A filter can have one matrix or a stack of matrices under it. Matrices under a filter are sometimes called <code>kernels</code>, but I will stick to the filter and matrix terms to avoid confusion.</p>
<p>So If we look at the LeNet-5 model configuration, we will find that we created three convolutional filters or <code>nn.Conv2d</code> layers. Let’s print the dimension of these layers.</p>
<div id="cell-26" class="cell" data-outputid="2cb9635a-6a3b-4400-d710-b2227d3570fd">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>model_weights <span class="op">=</span> [] </span>
<span id="cb16-2"><a href="#cb16-2"></a>conv_layers <span class="op">=</span> [] </span>
<span id="cb16-3"><a href="#cb16-3"></a>model_children <span class="op">=</span> <span class="bu">list</span>(model.children())</span>
<span id="cb16-4"><a href="#cb16-4"></a></span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="co"># counter to keep count of the conv layers</span></span>
<span id="cb16-6"><a href="#cb16-6"></a>counter <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb16-7"><a href="#cb16-7"></a><span class="co"># append all the conv layers and their respective weights to the list</span></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(model_children)):</span>
<span id="cb16-9"><a href="#cb16-9"></a>    <span class="cf">if</span> <span class="bu">type</span>(model_children[i]) <span class="op">==</span> nn.Conv2d:</span>
<span id="cb16-10"><a href="#cb16-10"></a>        counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-11"><a href="#cb16-11"></a>        model_weights.append(model_children[i].weight)</span>
<span id="cb16-12"><a href="#cb16-12"></a>        conv_layers.append(model_children[i])</span>
<span id="cb16-13"><a href="#cb16-13"></a></span>
<span id="cb16-14"><a href="#cb16-14"></a><span class="bu">print</span>(<span class="ss">f"Total convolutional layers: </span><span class="sc">{</span>counter<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-15"><a href="#cb16-15"></a></span>
<span id="cb16-16"><a href="#cb16-16"></a><span class="cf">for</span> weight, conv <span class="kw">in</span> <span class="bu">zip</span>(model_weights, conv_layers):</span>
<span id="cb16-17"><a href="#cb16-17"></a>    <span class="bu">print</span>(<span class="ss">f"CONV: </span><span class="sc">{</span>conv<span class="sc">}</span><span class="ss"> ====&gt; SHAPE: </span><span class="sc">{</span>weight<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total convolutional layers: 3
CONV: Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) ====&gt; SHAPE: torch.Size([6, 1, 5, 5])
CONV: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) ====&gt; SHAPE: torch.Size([16, 6, 5, 5])
CONV: Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1)) ====&gt; SHAPE: torch.Size([120, 16, 5, 5])</code></pre>
</div>
</div>
<p>The output of the above cell tells us that</p>
<ul>
<li>There are three convolutional layers</li>
<li>First layer has a dimension of <code>[6, 1, 5, 5]</code>. It means we have 6 filters in this layer. Each filter has 1 matrix of dimension 5x5 under it.</li>
<li>Second layer has a dimension of <code>[16, 6, 5, 5]</code>. It means we have 16 filters. Each filter has 6 matrices of size 5x5 under it.</li>
<li>Third layer has a dimension of <code>[120, 16, 5, 5]</code>. It means we have 120 filters. Each filter has 16 matrices of size 5x5 under it.</li>
</ul>
<p>Note that the learned weights from these matrices are stored in <code>model_weights</code> list, which we will visualize in the next section.</p>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co">##</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="co"># Helper function to visualize filters</span></span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="kw">def</span> visualize_filters(layer, n_plots<span class="op">=</span><span class="dv">30</span>):</span>
<span id="cb18-4"><a href="#cb18-4"></a>    <span class="co"># Get layer dimensions. e.g. `[6, 1, 5, 5]` where</span></span>
<span id="cb18-5"><a href="#cb18-5"></a>    <span class="co"># filters=6, kernels=1, kernel_height=5, kernel_weight=5</span></span>
<span id="cb18-6"><a href="#cb18-6"></a>    filters, kernels, kernel_height, kernel_weight <span class="op">=</span> layer.shape</span>
<span id="cb18-7"><a href="#cb18-7"></a>    <span class="co"># total plots = total number of matrices present in a layer. </span></span>
<span id="cb18-8"><a href="#cb18-8"></a>    <span class="co"># Each matrix weights can be plotted as an image</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>    total_plots <span class="op">=</span> filters <span class="op">*</span> kernels</span>
<span id="cb18-10"><a href="#cb18-10"></a>    <span class="co"># total_plots can be too many. So let's create an upper limit on them as 'MAX_PLOTS'</span></span>
<span id="cb18-11"><a href="#cb18-11"></a>    MAX_PLOTS <span class="op">=</span> <span class="bu">min</span>(n_plots, total_plots)</span>
<span id="cb18-12"><a href="#cb18-12"></a>    <span class="co"># number of columns for our plots 'MAX_COL_PLOTS'</span></span>
<span id="cb18-13"><a href="#cb18-13"></a>    MAX_COL_PLOTS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-14"><a href="#cb18-14"></a>    <span class="co"># number of rows for our plots 'MAX_ROW_PLOTS'</span></span>
<span id="cb18-15"><a href="#cb18-15"></a>    MAX_ROW_PLOTS <span class="op">=</span> <span class="bu">max</span>(MAX_PLOTS <span class="op">//</span> MAX_COL_PLOTS, MAX_COL_PLOTS)</span>
<span id="cb18-16"><a href="#cb18-16"></a></span>
<span id="cb18-17"><a href="#cb18-17"></a>    <span class="co"># specify some size of each plot image</span></span>
<span id="cb18-18"><a href="#cb18-18"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="fl">2.5</span><span class="op">*</span>MAX_ROW_PLOTS)) <span class="co"># width, height</span></span>
<span id="cb18-19"><a href="#cb18-19"></a>    plt.tight_layout()</span>
<span id="cb18-20"><a href="#cb18-20"></a>    </span>
<span id="cb18-21"><a href="#cb18-21"></a>    plot_count <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-22"><a href="#cb18-22"></a>    <span class="co"># interate filters</span></span>
<span id="cb18-23"><a href="#cb18-23"></a>    <span class="cf">for</span> f, <span class="bu">filter</span> <span class="kw">in</span> <span class="bu">enumerate</span>(layer):</span>
<span id="cb18-24"><a href="#cb18-24"></a>        <span class="co"># iterate kernels under each filter</span></span>
<span id="cb18-25"><a href="#cb18-25"></a>        <span class="cf">for</span> k, kernel <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">filter</span>):</span>
<span id="cb18-26"><a href="#cb18-26"></a>            <span class="co"># plot a single kernel or a matrix</span></span>
<span id="cb18-27"><a href="#cb18-27"></a>            plt.subplot(MAX_ROW_PLOTS, MAX_COL_PLOTS, plot_count)</span>
<span id="cb18-28"><a href="#cb18-28"></a>            plt.imshow(kernel[:, :].detach().cpu().numpy(), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb18-29"><a href="#cb18-29"></a>            plt.title(<span class="st">'#F:</span><span class="sc">{}</span><span class="st"> - K:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(f, k), {<span class="st">'size'</span>: <span class="dv">12</span>})</span>
<span id="cb18-30"><a href="#cb18-30"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb18-31"><a href="#cb18-31"></a>            plot_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-32"><a href="#cb18-32"></a>            </span>
<span id="cb18-33"><a href="#cb18-33"></a>            <span class="co"># terminate on `MAX_PLOTS` </span></span>
<span id="cb18-34"><a href="#cb18-34"></a>            <span class="cf">if</span> plot_count <span class="op">&gt;</span> MAX_PLOTS:</span>
<span id="cb18-35"><a href="#cb18-35"></a>                <span class="cf">return</span> plt.show()</span>
<span id="cb18-36"><a href="#cb18-36"></a>    </span>
<span id="cb18-37"><a href="#cb18-37"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="visualize-weights-for-first-conv2d-layer" class="level3">
<h3 class="anchored" data-anchor-id="visualize-weights-for-first-conv2d-layer">Visualize weights for first ‘Conv2d’ layer</h3>
<div id="cell-30" class="cell" data-outputid="bfbcd89c-2988-45f7-f1c0-9bc466726428">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>visualize_filters(model_weights[<span class="dv">0</span>], <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualize-weights-for-second-conv2d-layer" class="level3">
<h3 class="anchored" data-anchor-id="visualize-weights-for-second-conv2d-layer">Visualize weights for second ‘Conv2d’ layer</h3>
<div id="cell-32" class="cell" data-outputid="b9366b2b-d056-4f99-ce6b-c89c12a50d06">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>visualize_filters(model_weights[<span class="dv">1</span>], <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualize-weights-for-third-conv2d-layer" class="level3">
<h3 class="anchored" data-anchor-id="visualize-weights-for-third-conv2d-layer">Visualize weights for third ‘Conv2d’ layer</h3>
<div id="cell-34" class="cell" data-outputid="f3baec43-d595-4d7d-e9cc-6f3c40ac1760">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>visualize_filters(model_weights[<span class="dv">2</span>], <span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="what-do-these-filter-images-tell-us" class="level3">
<h3 class="anchored" data-anchor-id="what-do-these-filter-images-tell-us">What do these filter images tell us?</h3>
<p>These learned filter (2d matrix) images seem very random. How can these filters create an output (convolution or intermediate dataset) that can help our model to learn and classify an image to its correct class? Filters usually learn to find edges and curves from images. When these filters are applied to images, they amplify certain aspects of these images, like edges, curves, lines, or other patterns.</p>
<p>Looking just at the filters does not give us much information. So in the next section, we will visualize the outputs (or convolutions) produced by these filters.</p>
</section>
</section>
<section id="visualize-feature-maps-for-convolutional-layers" class="level2">
<h2 class="anchored" data-anchor-id="visualize-feature-maps-for-convolutional-layers">Visualize feature maps for convolutional layers</h2>
<p>The output produced by a neural net <code>layer</code> is called its feature map. These layers can be convolutional, flattening, or linear (fully connected). For example, in our LeNet-5 model, we have three convolutional layers, and each of these layers produces a feature map. In this section, we will visualize them.</p>
<section id="define-a-hook" class="level3">
<h3 class="anchored" data-anchor-id="define-a-hook">Define a hook</h3>
<p>A <code>hook</code> is simply a function we can give to our model to execute after its forward or backward pass. While attaching (or registering) a hook to a model, we provide the layer name on which we want to connect it. A hook function takes three arguments.</p>
<ul>
<li>a layer or a model</li>
<li>a tensor representing the inputs to the layer or model</li>
<li>a tensor representing the outputs from the layer or model</li>
</ul>
<p>Let’s define a function that will serve as our hook.</p>
<div id="cell-38" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co">##</span></span>
<span id="cb22-2"><a href="#cb22-2"></a><span class="co"># 'activation' is a dictionary to store the output from the layer</span></span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="co"># It should be defined outside the function, otherwise we will not be able to access it.</span></span>
<span id="cb22-4"><a href="#cb22-4"></a>activation <span class="op">=</span> {}</span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="kw">def</span> get_activation(name):</span>
<span id="cb22-6"><a href="#cb22-6"></a>    <span class="kw">def</span> hook(model, <span class="bu">input</span>, output):</span>
<span id="cb22-7"><a href="#cb22-7"></a>        activation[name] <span class="op">=</span> output.detach()</span>
<span id="cb22-8"><a href="#cb22-8"></a>    <span class="cf">return</span> hook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s also define a function to visualize the feature maps.</p>
<div id="cell-40" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="co">##</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="co"># Helper funtion to visualize the feature maps</span></span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="kw">def</span> visualize_feature_map(layer, n_plots<span class="op">=</span><span class="dv">30</span>, cmap<span class="op">=</span><span class="st">'gray'</span>, repeats<span class="op">=</span><span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>)):</span>
<span id="cb23-4"><a href="#cb23-4"></a>    <span class="co"># get feature map values and store them as 'act'</span></span>
<span id="cb23-5"><a href="#cb23-5"></a>    act <span class="op">=</span> layer.squeeze()</span>
<span id="cb23-6"><a href="#cb23-6"></a></span>
<span id="cb23-7"><a href="#cb23-7"></a>    <span class="co"># if feature map has three dimension</span></span>
<span id="cb23-8"><a href="#cb23-8"></a>    <span class="cf">if</span> <span class="bu">len</span>(act.shape) <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb23-9"><a href="#cb23-9"></a>        total_plots, plot_width, plot_height <span class="op">=</span> act.shape</span>
<span id="cb23-10"><a href="#cb23-10"></a></span>
<span id="cb23-11"><a href="#cb23-11"></a>        <span class="co"># total_plots can be too many so let's create an upper limit on them as 'MAX_PLOT'</span></span>
<span id="cb23-12"><a href="#cb23-12"></a>        MAX_PLOT <span class="op">=</span> <span class="bu">min</span>(total_plots, n_plots)</span>
<span id="cb23-13"><a href="#cb23-13"></a>        <span class="co"># number of columns for our plots as 'MAX_COL_PLOTS'</span></span>
<span id="cb23-14"><a href="#cb23-14"></a>        MAX_COL_PLOTS <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb23-15"><a href="#cb23-15"></a>        <span class="co"># number of rows for our plots as 'MAX_ROW_PLOTs'</span></span>
<span id="cb23-16"><a href="#cb23-16"></a>        MAX_ROW_PLOTs <span class="op">=</span> <span class="bu">max</span>(MAX_PLOT <span class="op">//</span> MAX_COL_PLOTS, MAX_COL_PLOTS)</span>
<span id="cb23-17"><a href="#cb23-17"></a></span>
<span id="cb23-18"><a href="#cb23-18"></a>        <span class="co"># specify some size for each plot image</span></span>
<span id="cb23-19"><a href="#cb23-19"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="fl">3.5</span><span class="op">*</span>MAX_ROW_PLOTs)) <span class="co"># width, height</span></span>
<span id="cb23-20"><a href="#cb23-20"></a>        plt.tight_layout()</span>
<span id="cb23-21"><a href="#cb23-21"></a></span>
<span id="cb23-22"><a href="#cb23-22"></a>        plot_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-23"><a href="#cb23-23"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(MAX_ROW_PLOTs):</span>
<span id="cb23-24"><a href="#cb23-24"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(MAX_COL_PLOTS):</span>
<span id="cb23-25"><a href="#cb23-25"></a>                plt.subplot(MAX_ROW_PLOTs, MAX_COL_PLOTS, plot_count<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb23-26"><a href="#cb23-26"></a>                plt.imshow(act[plot_count].detach().cpu().numpy(), cmap<span class="op">=</span>cmap)</span>
<span id="cb23-27"><a href="#cb23-27"></a>                plt.title(<span class="st">'#R:</span><span class="sc">{}</span><span class="st"> - C:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i, j), {<span class="st">'size'</span>: <span class="dv">12</span>})</span>
<span id="cb23-28"><a href="#cb23-28"></a>                plt.axis(<span class="st">'off'</span>)</span>
<span id="cb23-29"><a href="#cb23-29"></a>                plot_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-30"><a href="#cb23-30"></a></span>
<span id="cb23-31"><a href="#cb23-31"></a>                <span class="co"># terminate if plot_count reaches MAX_PLOT</span></span>
<span id="cb23-32"><a href="#cb23-32"></a>                <span class="cf">if</span> plot_count <span class="op">&gt;=</span> MAX_PLOT:</span>
<span id="cb23-33"><a href="#cb23-33"></a>                    <span class="cf">return</span> plt.show()</span>
<span id="cb23-34"><a href="#cb23-34"></a>    <span class="cf">else</span>: <span class="co"># len(act.shape) == 3</span></span>
<span id="cb23-35"><a href="#cb23-35"></a>    <span class="co"># if feature map has two dimension</span></span>
<span id="cb23-36"><a href="#cb23-36"></a>        arr_r <span class="op">=</span> np.repeat(act.reshape(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), repeats<span class="op">=</span>repeats, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-37"><a href="#cb23-37"></a>        plt.figure(figsize<span class="op">=</span>figsize) <span class="co"># width, height</span></span>
<span id="cb23-38"><a href="#cb23-38"></a>        plt.tight_layout()</span>
<span id="cb23-39"><a href="#cb23-39"></a>        plt.imshow(arr_r, cmap<span class="op">=</span>cmap)</span>
<span id="cb23-40"><a href="#cb23-40"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb23-41"><a href="#cb23-41"></a></span>
<span id="cb23-42"><a href="#cb23-42"></a>        <span class="cf">return</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="feature-map-from-first-conv2d-layer" class="level3">
<h3 class="anchored" data-anchor-id="feature-map-from-first-conv2d-layer">Feature map from first ‘Conv2d’ layer</h3>
<p>Let’s register our hook to first convolutional layer and visualize its feature map.</p>
<div id="cell-42" class="cell" data-outputid="72ac2a99-331b-42f9-b76f-bde9c15125d8">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co">##</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="co"># empty 'activation' as a precaution</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>activation <span class="op">=</span> {}</span>
<span id="cb24-4"><a href="#cb24-4"></a></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="co"># first 'Conv2d' layer is named as 'C1'</span></span>
<span id="cb24-6"><a href="#cb24-6"></a>handle <span class="op">=</span> model.C1.register_forward_hook(get_activation(<span class="st">'C1'</span>))</span>
<span id="cb24-7"><a href="#cb24-7"></a></span>
<span id="cb24-8"><a href="#cb24-8"></a><span class="co"># take any dataset image. image '7' is for three number</span></span>
<span id="cb24-9"><a href="#cb24-9"></a>data, label <span class="op">=</span> train_dataset[<span class="dv">7</span>]</span>
<span id="cb24-10"><a href="#cb24-10"></a>data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb24-11"><a href="#cb24-11"></a>output <span class="op">=</span> model(data)</span>
<span id="cb24-12"><a href="#cb24-12"></a></span>
<span id="cb24-13"><a href="#cb24-13"></a><span class="co"># remove hook</span></span>
<span id="cb24-14"><a href="#cb24-14"></a>handle.remove()</span>
<span id="cb24-15"><a href="#cb24-15"></a></span>
<span id="cb24-16"><a href="#cb24-16"></a><span class="bu">print</span>(<span class="ss">f"Dimensions for C1 feature map: </span><span class="sc">{</span>activation[<span class="st">'C1'</span>]<span class="sc">.</span>squeeze()<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions for C1 feature map: torch.Size([6, 28, 28])</code></pre>
</div>
</div>
<div id="cell-43" class="cell" data-outputid="999beefd-ccf2-483e-bfc1-c6fc2eaadb27">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="co">##</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="co"># visualize the feature map</span></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="co"># dimensions: 6@28x28</span></span>
<span id="cb26-4"><a href="#cb26-4"></a>visualize_feature_map(activation[<span class="st">'C1'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="feature-map-from-second-conv2d-layer" class="level3">
<h3 class="anchored" data-anchor-id="feature-map-from-second-conv2d-layer">Feature map from second ‘Conv2d’ layer</h3>
<p>Let’s register our hook to second convolutional layer and visualize its feature map.</p>
<div id="cell-45" class="cell" data-outputid="7ebf2cb9-5e51-48e4-f458-501003a2a629">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co">##</span></span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="co"># empty 'activation' as a precaution</span></span>
<span id="cb27-3"><a href="#cb27-3"></a>activation <span class="op">=</span> {}</span>
<span id="cb27-4"><a href="#cb27-4"></a></span>
<span id="cb27-5"><a href="#cb27-5"></a><span class="co"># second 'Conv2d' layer is named as 'C3'</span></span>
<span id="cb27-6"><a href="#cb27-6"></a>handle <span class="op">=</span> model.C3.register_forward_hook(get_activation(<span class="st">'C3'</span>))</span>
<span id="cb27-7"><a href="#cb27-7"></a></span>
<span id="cb27-8"><a href="#cb27-8"></a><span class="co"># take any dataset image. image '7' is for three number</span></span>
<span id="cb27-9"><a href="#cb27-9"></a>data, label <span class="op">=</span> train_dataset[<span class="dv">7</span>]</span>
<span id="cb27-10"><a href="#cb27-10"></a>data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb27-11"><a href="#cb27-11"></a>output <span class="op">=</span> model(data)</span>
<span id="cb27-12"><a href="#cb27-12"></a></span>
<span id="cb27-13"><a href="#cb27-13"></a><span class="co"># remove hook</span></span>
<span id="cb27-14"><a href="#cb27-14"></a>handle.remove()</span>
<span id="cb27-15"><a href="#cb27-15"></a></span>
<span id="cb27-16"><a href="#cb27-16"></a><span class="bu">print</span>(<span class="ss">f"Dimensions for C3 feature map: </span><span class="sc">{</span>activation[<span class="st">'C3'</span>]<span class="sc">.</span>squeeze()<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions for C3 feature map: torch.Size([16, 10, 10])</code></pre>
</div>
</div>
<div id="cell-46" class="cell" data-outputid="318fe723-f8bc-4c23-b59a-42e492954427">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="co">##</span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="co"># visualize the feature map</span></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="co"># dimensions: 16@10x10</span></span>
<span id="cb29-4"><a href="#cb29-4"></a>visualize_feature_map(activation[<span class="st">'C3'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="feature-maps-from-the-first-and-second-conv2d-layers-together" class="level3">
<h3 class="anchored" data-anchor-id="feature-maps-from-the-first-and-second-conv2d-layers-together">Feature maps from the first and second ‘Conv2d’ layers together</h3>
<p>Feature maps from the first layer show that they are sharper. Feature maps from the second layer show that they are more spread out or convolved. By spreading out or blurring effect, it seems like only the most significant features remain in the output, and the rest slowly disappear. For example, in the case of ‘3’ in the second feature map, only the horizontal edge signal remains, and any other signal gets dissolved.</p>
<p>To get more intuition of what is happening here, let’s visualize both feature maps together and for multiple images.</p>
<div id="cell-48" class="cell" data-outputid="c9adc2f0-8e45-44f0-ef35-896f88a9ecf3">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co">##</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="co"># Visualize feature maps for C1 and C3 together</span></span>
<span id="cb30-3"><a href="#cb30-3"></a><span class="co"># Visualize them for first 5 train images</span></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb30-5"><a href="#cb30-5"></a>    <span class="co"># just a separator.</span></span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss"> IMAGE </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-7"><a href="#cb30-7"></a></span>
<span id="cb30-8"><a href="#cb30-8"></a>    <span class="co"># empty 'activation' as a precaution</span></span>
<span id="cb30-9"><a href="#cb30-9"></a>    activation <span class="op">=</span> {}</span>
<span id="cb30-10"><a href="#cb30-10"></a></span>
<span id="cb30-11"><a href="#cb30-11"></a>    <span class="co"># create hooks for C1 and C3</span></span>
<span id="cb30-12"><a href="#cb30-12"></a>    handle1 <span class="op">=</span> model.C1.register_forward_hook(get_activation(<span class="st">'C1'</span>))</span>
<span id="cb30-13"><a href="#cb30-13"></a>    handle2 <span class="op">=</span> model.C3.register_forward_hook(get_activation(<span class="st">'C3'</span>))</span>
<span id="cb30-14"><a href="#cb30-14"></a>    </span>
<span id="cb30-15"><a href="#cb30-15"></a>    data, _ <span class="op">=</span> train_dataset[i]</span>
<span id="cb30-16"><a href="#cb30-16"></a>    data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb30-17"><a href="#cb30-17"></a>    output <span class="op">=</span> model(data)</span>
<span id="cb30-18"><a href="#cb30-18"></a></span>
<span id="cb30-19"><a href="#cb30-19"></a>    <span class="co"># remove hooks</span></span>
<span id="cb30-20"><a href="#cb30-20"></a>    handle1.remove()</span>
<span id="cb30-21"><a href="#cb30-21"></a>    handle2.remove()</span>
<span id="cb30-22"><a href="#cb30-22"></a></span>
<span id="cb30-23"><a href="#cb30-23"></a>    <span class="co"># visualize feature maps</span></span>
<span id="cb30-24"><a href="#cb30-24"></a>    <span class="co"># I have chaged the colors of output to sharpen the differences</span></span>
<span id="cb30-25"><a href="#cb30-25"></a>    visualize_feature_map(activation[<span class="st">'C1'</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb30-26"><a href="#cb30-26"></a>    visualize_feature_map(activation[<span class="st">'C3'</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** IMAGE 0 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** IMAGE 1 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** IMAGE 2 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** IMAGE 3 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** IMAGE 4 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-24-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="feature-map-from-third-conv2d-layer" class="level3">
<h3 class="anchored" data-anchor-id="feature-map-from-third-conv2d-layer">Feature map from third ‘Conv2d’ layer</h3>
<p>Let’s register our hook to third convolutional layer and visualize its feature map.</p>
<div id="cell-50" class="cell" data-outputid="51199546-fada-4758-c2f4-3fbd26998927">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="co">##</span></span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="co"># empty 'activation' as a precaution</span></span>
<span id="cb36-3"><a href="#cb36-3"></a>activation <span class="op">=</span> {}</span>
<span id="cb36-4"><a href="#cb36-4"></a></span>
<span id="cb36-5"><a href="#cb36-5"></a><span class="co"># third 'Conv2d' layer is named as 'C5'</span></span>
<span id="cb36-6"><a href="#cb36-6"></a>handle <span class="op">=</span> model.C5.register_forward_hook(get_activation(<span class="st">'C5'</span>))</span>
<span id="cb36-7"><a href="#cb36-7"></a></span>
<span id="cb36-8"><a href="#cb36-8"></a><span class="co"># take any dataset image. image '7' is for three number</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>data, label <span class="op">=</span> train_dataset[<span class="dv">7</span>]</span>
<span id="cb36-10"><a href="#cb36-10"></a>data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb36-11"><a href="#cb36-11"></a>output <span class="op">=</span> model(data)</span>
<span id="cb36-12"><a href="#cb36-12"></a></span>
<span id="cb36-13"><a href="#cb36-13"></a><span class="co"># remove hook</span></span>
<span id="cb36-14"><a href="#cb36-14"></a>handle.remove()</span>
<span id="cb36-15"><a href="#cb36-15"></a></span>
<span id="cb36-16"><a href="#cb36-16"></a><span class="bu">print</span>(<span class="ss">f"Dimensions for C5 feature map: </span><span class="sc">{</span>activation[<span class="st">'C5'</span>]<span class="sc">.</span>squeeze()<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions for C5 feature map: torch.Size([120])</code></pre>
</div>
</div>
<div id="cell-51" class="cell" data-outputid="d8f8982a-9231-47e7-9430-b41f3e81edda">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="co">##</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="co"># visualize the feature map</span></span>
<span id="cb38-3"><a href="#cb38-3"></a><span class="co"># dimensions: 120@1x1</span></span>
<span id="cb38-4"><a href="#cb38-4"></a>visualize_feature_map(activation[<span class="st">'C5'</span>], figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">15</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="feature-map-from-third-conv2d-layer-for-multiple-images" class="level3">
<h3 class="anchored" data-anchor-id="feature-map-from-third-conv2d-layer-for-multiple-images">Feature map from third ‘Conv2d’ layer for multiple images</h3>
<p>The output from the third ‘Conv2d’ layer <code>C5</code> looks like a signature bar code that the model learns to associate with a particular target class. Let’s visualize more of them for a set of 1, 3, and 5 images together.</p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="co">##</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="co"># create a collection of 1, 3, and 5 images</span></span>
<span id="cb39-3"><a href="#cb39-3"></a>bucket <span class="op">=</span> {<span class="st">'1'</span>:[], <span class="st">'3'</span>:[], <span class="st">'5'</span>:[]}</span>
<span id="cb39-4"><a href="#cb39-4"></a></span>
<span id="cb39-5"><a href="#cb39-5"></a><span class="co"># iterate through the dataset till we have 5 images for each class</span></span>
<span id="cb39-6"><a href="#cb39-6"></a>count_1, count_3, count_5 <span class="op">=</span> <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span></span>
<span id="cb39-7"><a href="#cb39-7"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb39-8"><a href="#cb39-8"></a>    _, label <span class="op">=</span> train_dataset[idx]</span>
<span id="cb39-9"><a href="#cb39-9"></a></span>
<span id="cb39-10"><a href="#cb39-10"></a>    <span class="cf">if</span> label <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> count_1 <span class="op">&lt;</span> <span class="dv">5</span>:</span>
<span id="cb39-11"><a href="#cb39-11"></a>        bucket[<span class="bu">str</span>(label)].append(idx)</span>
<span id="cb39-12"><a href="#cb39-12"></a>        count_1 <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb39-13"><a href="#cb39-13"></a>    <span class="cf">elif</span> label <span class="op">==</span> <span class="dv">3</span> <span class="kw">and</span> count_3 <span class="op">&lt;</span> <span class="dv">5</span>:</span>
<span id="cb39-14"><a href="#cb39-14"></a>        bucket[<span class="bu">str</span>(label)].append(idx)</span>
<span id="cb39-15"><a href="#cb39-15"></a>        count_3 <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb39-16"><a href="#cb39-16"></a>    <span class="cf">elif</span> label <span class="op">==</span> <span class="dv">5</span> <span class="kw">and</span> count_5 <span class="op">&lt;</span> <span class="dv">5</span>:</span>
<span id="cb39-17"><a href="#cb39-17"></a>        bucket[<span class="bu">str</span>(label)].append(idx)</span>
<span id="cb39-18"><a href="#cb39-18"></a>        count_5 <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb39-19"><a href="#cb39-19"></a></span>
<span id="cb39-20"><a href="#cb39-20"></a>    <span class="cf">if</span> count_1 <span class="op">+</span> count_3 <span class="op">+</span> count_5 <span class="op">==</span> <span class="dv">15</span>:</span>
<span id="cb39-21"><a href="#cb39-21"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-54" class="cell" data-outputid="f70c679b-fc14-4303-8625-3d1d051b495a">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="co">##</span></span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="co"># visualize feature maps for a set of images from class 1, 3 and 5</span></span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="cf">for</span> key <span class="kw">in</span> bucket:</span>
<span id="cb40-4"><a href="#cb40-4"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss"> LAYER C5, LABEL </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-5"><a href="#cb40-5"></a></span>
<span id="cb40-6"><a href="#cb40-6"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bucket[key])):</span>
<span id="cb40-7"><a href="#cb40-7"></a>        activation <span class="op">=</span> {}</span>
<span id="cb40-8"><a href="#cb40-8"></a>        </span>
<span id="cb40-9"><a href="#cb40-9"></a>        <span class="co"># attach hook</span></span>
<span id="cb40-10"><a href="#cb40-10"></a>        handle <span class="op">=</span> model.C5.register_forward_hook(get_activation(<span class="st">'C5'</span>))</span>
<span id="cb40-11"><a href="#cb40-11"></a>        idx <span class="op">=</span> bucket[key][i]</span>
<span id="cb40-12"><a href="#cb40-12"></a>        data, label <span class="op">=</span> train_dataset[idx]</span>
<span id="cb40-13"><a href="#cb40-13"></a>        data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb40-14"><a href="#cb40-14"></a>        output <span class="op">=</span> model(data)</span>
<span id="cb40-15"><a href="#cb40-15"></a>        </span>
<span id="cb40-16"><a href="#cb40-16"></a>        <span class="co"># remove hook</span></span>
<span id="cb40-17"><a href="#cb40-17"></a>        handle.remove()</span>
<span id="cb40-18"><a href="#cb40-18"></a>        </span>
<span id="cb40-19"><a href="#cb40-19"></a>        <span class="co"># visualize feature map</span></span>
<span id="cb40-20"><a href="#cb40-20"></a>        <span class="co"># i have changed the output colormap to sharpen the differences</span></span>
<span id="cb40-21"><a href="#cb40-21"></a>        visualize_feature_map(activation[<span class="st">'C5'</span>], cmap<span class="op">=</span><span class="st">'hsv'</span>, figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">15</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER C5, LABEL 1 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER C5, LABEL 3 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER C5, LABEL 5 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-16.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-17.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-28-output-18.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="visualize-feature-maps-for-a-classifier-or-hidden-layer" class="level2">
<h2 class="anchored" data-anchor-id="visualize-feature-maps-for-a-classifier-or-hidden-layer">Visualize feature maps for a classifier or hidden layer</h2>
<p>We have two hidden layers or ‘nn.Linear’ in the classification part of our model. They are also called fully connected layers. Let’s visualize the feature maps for them.</p>
<section id="visualize-feature-map-for-first-linear-layer" class="level3">
<h3 class="anchored" data-anchor-id="visualize-feature-map-for-first-linear-layer">Visualize feature map for first ‘Linear’ layer</h3>
<p>Let’s visualize the feature maps for the first hidden layer.</p>
<div id="cell-57" class="cell" data-outputid="3fea22a4-aa66-4afb-99a5-23628b02dd6a">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># visualize differences for images 1,3 and 5</span></span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="cf">for</span> key <span class="kw">in</span> bucket:</span>
<span id="cb44-3"><a href="#cb44-3"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss"> LAYER F6, LABEL </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-4"><a href="#cb44-4"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bucket[key])):</span>
<span id="cb44-5"><a href="#cb44-5"></a></span>
<span id="cb44-6"><a href="#cb44-6"></a>        activation <span class="op">=</span> {}</span>
<span id="cb44-7"><a href="#cb44-7"></a>        handle1 <span class="op">=</span> model.F6.register_forward_hook(get_activation(<span class="st">'F6'</span>))</span>
<span id="cb44-8"><a href="#cb44-8"></a>        idx <span class="op">=</span> bucket[key][i]</span>
<span id="cb44-9"><a href="#cb44-9"></a>        data, label <span class="op">=</span> train_dataset[idx]</span>
<span id="cb44-10"><a href="#cb44-10"></a>        </span>
<span id="cb44-11"><a href="#cb44-11"></a>        data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb44-12"><a href="#cb44-12"></a>        output <span class="op">=</span> model(data)</span>
<span id="cb44-13"><a href="#cb44-13"></a>        handle1.remove()</span>
<span id="cb44-14"><a href="#cb44-14"></a>        </span>
<span id="cb44-15"><a href="#cb44-15"></a>        visualize_feature_map(activation[<span class="st">'F6'</span>], cmap<span class="op">=</span><span class="st">'hsv'</span>, figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">15</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER F6, LABEL 1 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER F6, LABEL 3 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER F6, LABEL 5 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-16.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-17.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-29-output-18.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualize-feature-map-for-second-linear-layer" class="level3">
<h3 class="anchored" data-anchor-id="visualize-feature-map-for-second-linear-layer">Visualize feature map for second ‘Linear’ layer</h3>
<p>Let’s visualize the feature maps for the second hidden layer.</p>
<div id="cell-59" class="cell" data-outputid="99b189b7-1597-4422-f5ae-6bd72c905823">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a><span class="co"># visualize differences for images 1,3 and 5</span></span>
<span id="cb48-2"><a href="#cb48-2"></a><span class="cf">for</span> key <span class="kw">in</span> bucket:</span>
<span id="cb48-3"><a href="#cb48-3"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss"> LAYER OUTPUT, LABEL </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'*'</span><span class="op">*</span><span class="dv">30</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-4"><a href="#cb48-4"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bucket[key])):</span>
<span id="cb48-5"><a href="#cb48-5"></a></span>
<span id="cb48-6"><a href="#cb48-6"></a>        activation <span class="op">=</span> {}</span>
<span id="cb48-7"><a href="#cb48-7"></a>        handle1 <span class="op">=</span> model.OUTPUT.register_forward_hook(get_activation(<span class="st">'OUTPUT'</span>))</span>
<span id="cb48-8"><a href="#cb48-8"></a>        idx <span class="op">=</span> bucket[key][i]</span>
<span id="cb48-9"><a href="#cb48-9"></a>        data, label <span class="op">=</span> train_dataset[idx]</span>
<span id="cb48-10"><a href="#cb48-10"></a>        </span>
<span id="cb48-11"><a href="#cb48-11"></a>        data.unsqueeze_(<span class="dv">0</span>)</span>
<span id="cb48-12"><a href="#cb48-12"></a>        output <span class="op">=</span> model(data)</span>
<span id="cb48-13"><a href="#cb48-13"></a>        handle1.remove()</span>
<span id="cb48-14"><a href="#cb48-14"></a>        </span>
<span id="cb48-15"><a href="#cb48-15"></a>        visualize_feature_map(activation[<span class="st">'OUTPUT'</span>], repeats<span class="op">=</span><span class="dv">1</span>,cmap<span class="op">=</span><span class="st">'hsv'</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER OUTPUT, LABEL 1 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER OUTPUT, LABEL 3 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>****************************** LAYER OUTPUT, LABEL 5 ******************************</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-16.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-17.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-30-output-18.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="what-do-these-output-feature-maps-tell-us" class="level3">
<h3 class="anchored" data-anchor-id="what-do-these-output-feature-maps-tell-us">What do these OUTPUT feature maps tell us?</h3>
<p>We can see that output feature maps of the same numbers are very similar. They show that our model has learned some hidden patterns that distinguish these images. For example</p>
<ul>
<li>For label 1, all output feature maps have a RED bar at the extreme left and a BLUE bar at the right side</li>
<li>For label 3, there are two RED bars around the middle and a light BLUE bar running along them</li>
<li>For label 5, RED bars are in the middle and at the extreme right side.</li>
</ul>
<p>If we look at the first feature map from label 5, it is a bit different. A kind of outlier from the rest of label 5 feature maps. What could be the reason for this?</p>
<p>Let’s plot these label 5 images to check their actual shape.</p>
<div id="cell-61" class="cell" data-outputid="f1054235-4287-43b9-b337-749fc40acfee">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a><span class="co">##</span></span>
<span id="cb52-2"><a href="#cb52-2"></a><span class="co"># label 5 image indexes</span></span>
<span id="cb52-3"><a href="#cb52-3"></a>bucket[<span class="st">'5'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>[0, 11, 35, 47, 65]</code></pre>
</div>
</div>
<div id="cell-62" class="cell" data-outputid="b4664a3c-3a2c-4dff-8e95-211a8619965e">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="co">##</span></span>
<span id="cb54-2"><a href="#cb54-2"></a><span class="co"># Plot label 5 images</span></span>
<span id="cb54-3"><a href="#cb54-3"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb54-4"><a href="#cb54-4"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(bucket[<span class="st">'5'</span>]):</span>
<span id="cb54-5"><a href="#cb54-5"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb54-6"><a href="#cb54-6"></a>    plt.imshow(train_dataset.data[idx], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb54-7"><a href="#cb54-7"></a>    plt.title(<span class="ss">f"#Index:</span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss"> "</span>)</span>
<span id="cb54-8"><a href="#cb54-8"></a>    plt.axis(<span class="st">'off'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-10-18-pytorch-mnist-convolutional-neural-networks_files/figure-html/cell-32-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now we can see the reason for the output feature map of the first label ‘5’ image being quite different from the rest. The first image from the left side is slightly different from the rest. So it is a bit weird number five image, and that weirdness got reflected in the feature map generated by it.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="hassaanbinaslam/myblog_utterances" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>