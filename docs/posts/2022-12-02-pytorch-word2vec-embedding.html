<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-12-02">
<meta name="description" content="This is a practice notebook to implement word2vec in PyTorch.">

<title>Random Thoughts - Implementing Word2Vec with PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-20316028', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Random Thoughts - Implementing Word2Vec with PyTorch">
<meta property="og:description" content="This is a practice notebook to implement word2vec in PyTorch.">
<meta property="og:image" content="images/2022-12-02-pytorch-word2vec-embedding.png">
<meta property="og:site-name" content="Random Thoughts">
<meta name="twitter:title" content="Random Thoughts - Implementing Word2Vec with PyTorch">
<meta name="twitter:description" content="This is a practice notebook to implement word2vec in PyTorch.">
<meta name="twitter:image" content="images/2022-12-02-pytorch-word2vec-embedding.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Random Thoughts</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hassaanbinaslam/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassaanbinaslam/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hassaanbinaslam"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#credits" id="toc-credits" class="nav-link active" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a>
  <ul class="collapse">
  <li><a href="#load-hugging-face-dataset" id="toc-load-hugging-face-dataset" class="nav-link" data-scroll-target="#load-hugging-face-dataset">Load Hugging Face Dataset</a></li>
  <li><a href="#preprocess-data" id="toc-preprocess-data" class="nav-link" data-scroll-target="#preprocess-data">Preprocess data</a></li>
  <li><a href="#create-vocabulary" id="toc-create-vocabulary" class="nav-link" data-scroll-target="#create-vocabulary">Create vocabulary</a></li>
  <li><a href="#create-training-samples-using-skip-gram" id="toc-create-training-samples-using-skip-gram" class="nav-link" data-scroll-target="#create-training-samples-using-skip-gram">Create training samples using skip-gram</a></li>
  <li><a href="#create-dataset-and-dataloader" id="toc-create-dataset-and-dataloader" class="nav-link" data-scroll-target="#create-dataset-and-dataloader">Create dataset and dataloader</a></li>
  </ul></li>
  <li><a href="#model-configuration" id="toc-model-configuration" class="nav-link" data-scroll-target="#model-configuration">Model Configuration</a></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training">Model Training</a></li>
  <li><a href="#analyze-word-embeddings-word-vectors" id="toc-analyze-word-embeddings-word-vectors" class="nav-link" data-scroll-target="#analyze-word-embeddings-word-vectors">Analyze Word Embeddings (Word Vectors)</a>
  <ul class="collapse">
  <li><a href="#visualize-embeddings" id="toc-visualize-embeddings" class="nav-link" data-scroll-target="#visualize-embeddings">Visualize embeddings</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Implementing Word2Vec with PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">nlp</div>
  </div>
  </div>

<div>
  <div class="description">
    This is a practice notebook to implement word2vec in PyTorch.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 2, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="images/2022-12-02-pytorch-word2vec-embedding.png" class="img-fluid"></p>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>This notebook takes inspiration and ideas from the following sources.</p>
<ul>
<li>An excellent word2vec introduction from “Jay Alammar”: <a href="https://jalammar.github.io/illustrated-word2vec/">illustrated-word2vec</a>.</li>
<li>Blog post by “Musashi (sometimes Jacobs-) Harukawa” with the same title. You can find the original post here: <a href="https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html">word2vec-from-scratch</a>. Parts of the code you see in this notebook are taken from this post.</li>
<li>Another very detailed and well-explained blog post by “Olga Chernytska”. You can find the original post here: <a href="https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0">word2vec-with-pytorch-implementing-original-paper</a>. Parts of the code you see in this notebook are taken from this post.</li>
</ul>
</section>
<section id="environment" class="level2">
<h2 class="anchored" data-anchor-id="environment">Environment</h2>
<p>This notebook <a href="">GitHub link here</a> is prepared with Google Colab. <a href="https://huggingface.co/docs/datasets/index">Hugging Face Datasets</a> library is required for this post.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:4182,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327669270,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="4bef8920-d475-4392-c626-eff7bec46cd2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># install hugging face datasets</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="op">!</span> pip install datasets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1902,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327671155,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="88ceae6c-0eb7-4b3b-b23e-eddf05a0cfe0" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> platform <span class="im">import</span> python_version</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> numpy, matplotlib, pandas, torch, datasets</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="bu">print</span>(<span class="st">"python=="</span> <span class="op">+</span> python_version())</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="bu">print</span>(<span class="st">"numpy=="</span> <span class="op">+</span> numpy.__version__)</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="bu">print</span>(<span class="st">"torch=="</span> <span class="op">+</span> torch.__version__)</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="bu">print</span>(<span class="st">"matplotlib=="</span> <span class="op">+</span> matplotlib.__version__)</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="bu">print</span>(<span class="st">"datasets=="</span> <span class="op">+</span> datasets.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>python==3.8.15
numpy==1.21.6
torch==1.12.1+cu113
matplotlib==3.2.2
datasets==2.7.1</code></pre>
</div>
</div>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<blockquote class="blockquote">
<p>Following are the <code>word2vec</code> and <code>word embedding</code> explanations provided by <a href="https://chat.openai.com/chat">OpenAI ChatGPT</a>. If unsatisfied, I suggest using the links provided in the “Credits” section (<a href="https://jalammar.github.io/illustrated-word2vec/">illustrated-word2vec</a> from Jay Alammar).</p>
</blockquote>
<p><code>prompt &gt; word2vec blog introduction</code></p>
<p><strong>Word2vec</strong></p>
<p>Word2vec is a widely-used natural language processing (NLP) algorithm that uses deep learning to learn the relationships between words in a corpus (a large collection of text data). A team of researchers developed it at Google, led by Tomas Mikolov in 2013.</p>
<p>Word2vec uses a neural network to learn the relationships between words by training on a large corpus of text data. The algorithm uses a shallow neural network with a single hidden layer to predict the surrounding words of a given word based on its context. The network’s hidden layer is trained to capture the relationships between words, and the resulting vectors (or “word embeddings”) can be used to represent the words in the corpus in a lower-dimensional space.</p>
<p>The benefits of using word2vec include the ability to capture the relationships between words in a corpus and the ability to handle large amounts of data efficiently. This makes it a useful tool for many NLP tasks, such as text classification, language translation and named entity recognition.</p>
<p>Overall, word2vec is a powerful and effective algorithm for learning the relationships between words in a corpus and has become a cornerstone of modern NLP techniques.</p>
<p><code>prompt &gt; word embedding short description</code></p>
<p><strong>Word Embedding</strong></p>
<p>A word embedding represents a word in a low-dimensional, continuous vector space. This representation captures the semantic relationships between words in a corpus, allowing for more accurate and efficient natural language processing. Word embeddings are typically learned using neural network models, such as the word2vec algorithm. These learned embeddings can then be used in various NLP tasks, such as text classification and language translation.</p>
<p><strong>Summary of the steps followed in this notebook</strong></p>
<ul>
<li>Download hugging face <a href="https://huggingface.co/datasets/tweets_hate_speech_detection">tweets_hate_speech_detection</a> dataset</li>
<li>Preprocess the data to remove special characters and normalize the text</li>
<li>Create tokens from the text and their vocabulary, and finally encode the tokens</li>
<li>Create training dataset, model configuration, loss, and optimizer</li>
<li>Train the model and extract the embedding layer weights</li>
<li>Analyse the word embeddings learned from the tweets</li>
</ul>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<section id="load-hugging-face-dataset" class="level3">
<h3 class="anchored" data-anchor-id="load-hugging-face-dataset">Load Hugging Face Dataset</h3>
<p>For this notebook, I will use Hugging Face <a href="https://huggingface.co/datasets/tweets_hate_speech_detection">Twitter Hate Speech Dataset</a>. The data contain training data with approximately 32K tweets divided into two groups: <code>hate speech</code> and `not a hate speech. This dataset is originally designed for classification tasks, but we may use it to learn word embeddings (or contexts). This approach can also be used to identify inherent biases present in the data.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2234,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673382,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="16456549-3281-4124-aea6-451e98eeaf01" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> datasets</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>dataset <span class="op">=</span> datasets.load_dataset(<span class="st">"tweets_hate_speech_detection"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.builder:Found cached dataset tweets_hate_speech_detection (/root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fd43128fc4d74e10a6d4d64df3d3195a","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:34,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673383,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="fb6fd427-9d13-4044-c9e0-a381d6d5eb21" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Let's check the downloaded object</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['label', 'tweet'],
        num_rows: 31962
    })
})</code></pre>
</div>
</div>
<p>It shows that we have only a training set, and each element from the set has a <code>label</code> and <code>tweet</code> text.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:32,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673385,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="5e54a2b7-279a-409d-d906-66e551d1c071" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>train_ds <span class="op">=</span> dataset[<span class="st">"train"</span>]</span>
<span id="cb8-2"><a href="#cb8-2"></a>train_ds.features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'label': ClassLabel(names=['no-hate-speech', 'hate-speech'], id=None),
 'tweet': Value(dtype='string', id=None)}</code></pre>
</div>
</div>
<p>Let’s check what the raw data look’s like.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673387,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="1a57b624-37ba-4873-ba11-745ce4576d7c" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># print a few labels</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="bu">print</span>(<span class="st">"** labels **</span><span class="ch">\n</span><span class="st">"</span>, train_ds[<span class="st">"label"</span>][:<span class="dv">5</span>])</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co"># print a few tweet texts</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">** tweets **"</span>)</span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="cf">for</span> t <span class="kw">in</span> train_ds[<span class="st">"tweet"</span>][:<span class="dv">5</span>]:</span>
<span id="cb10-7"><a href="#cb10-7"></a>    <span class="bu">print</span>(t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>** labels **
 [0, 0, 0, 0, 0]

** tweets **
@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run
@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked
bihday your majesty
#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  
factsguide: society now    #motivation</code></pre>
</div>
</div>
<p>This raw data shows that the labels are encoded as <code>0</code> and <code>1</code> for <code>no-hate</code> and <code>hate</code> speech, respectively. Therefore, we can improve our view by putting labels and tweets in Pandas’s DataFrame and analyzing them side by side.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673391,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="2424417e-661f-42e1-edfd-767bf7110512" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a>pd.set_option(<span class="st">"display.max_colwidth"</span>, <span class="va">None</span>)</span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a>train_ds.set_format(<span class="bu">type</span><span class="op">=</span><span class="st">"pandas"</span>)</span>
<span id="cb12-6"><a href="#cb12-6"></a>df <span class="op">=</span> train_ds[:]</span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="co"># a function to convert label codes to string value</span></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="kw">def</span> label_int2str(row):</span>
<span id="cb12-10"><a href="#cb12-10"></a>    <span class="cf">return</span> train_ds.features[<span class="st">"label"</span>].int2str(row)</span>
<span id="cb12-11"><a href="#cb12-11"></a></span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a>df[<span class="st">"label_name"</span>] <span class="op">=</span> df[<span class="st">"label"</span>].<span class="bu">apply</span>(label_int2str)</span>
<span id="cb12-14"><a href="#cb12-14"></a>df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">


  <div id="df-61dd778d-af34-4e0c-affc-b54453c4d2c8">
    <div class="colab-df-container">
      <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>label</th>
      <th>tweet</th>
      <th>label_name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>bihday your majesty</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>
      <td>no-hate-speech</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>
      <td>no-hate-speech</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-61dd778d-af34-4e0c-affc-b54453c4d2c8')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-61dd778d-af34-4e0c-affc-b54453c4d2c8 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-61dd778d-af34-4e0c-affc-b54453c4d2c8');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<p>All these tweets are labeled as <code>no-hate-speech</code>. Let’s view some of the tweets from the other class.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673393,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="088ea4cc-f69c-4f7a-c898-d26f56c03500" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>df[df[<span class="st">"label"</span>]<span class="op">==</span><span class="dv">1</span>].head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">


  <div id="df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3">
    <div class="colab-df-container">
      <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>label</th>
      <th>tweet</th>
      <th>label_name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1</td>
      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>17</th>
      <td>1</td>
      <td>retweet if you agree!</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1</td>
      <td>@user @user lumpy says i am a . prove it lumpy.</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>34</th>
      <td>1</td>
      <td>it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>56</th>
      <td>1</td>
      <td>@user lets fight against  #love #peace</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>68</th>
      <td>1</td>
      <td>ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>77</th>
      <td>1</td>
      <td>@user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>82</th>
      <td>1</td>
      <td>how the #altright uses  &amp;amp; insecurity to lure men into #whitesupremacy</td>
      <td>hate-speech</td>
    </tr>
    <tr>
      <th>111</th>
      <td>1</td>
      <td>@user i'm not interested in a #linguistics that doesn't address #race &amp;amp; . racism is about #power. #raciolinguistics bringsâ¦</td>
      <td>hate-speech</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327673394,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># reset dataset to its original format</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>train_ds.reset_format()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preprocess-data" class="level3">
<h3 class="anchored" data-anchor-id="preprocess-data">Preprocess data</h3>
<p>We can not use raw text directly to train a model because machines understand numbers and not alphabets. But to properly encode our text (convert it into numbers), we need to do some extra steps.</p>
<ul>
<li>tweets also contain special characters from emoticons or emojis. However, they do not help learn word embeddings. So we need to strip them from the text.</li>
<li>split the tweet text into proper words. Even though we can use all the words from the tweet text, experience has shown that not all are useful in learning embeddings. Words that are commonly omitted are either uncommon or rare words, or stopwords (commonly used words)</li>
<li>Create a dictionary or vocabulary to filter words and encode them. This vocabulary helps move between encoded (integer) and character (or string) representations of words.</li>
</ul>
<p>For some of the preprocessing tasks, we will use <a href="https://www.nltk.org/">Natural Language Toolkit (NLTK library)</a>. &gt; NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:914,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674278,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="b93b19a3-9300-49fe-f5d1-51df4f0d8882" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="im">import</span> nltk</span>
<span id="cb15-2"><a href="#cb15-2"></a></span>
<span id="cb15-3"><a href="#cb15-3"></a>nltk.download(<span class="st">"stopwords"</span>)  <span class="co"># for filtering common words</span></span>
<span id="cb15-4"><a href="#cb15-4"></a>nltk.download(<span class="st">"wordnet"</span>)  <span class="co"># for lemmatization of words</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>nltk.download(<span class="st">"omw-1.4"</span>)  <span class="co"># required for 'wordnet'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /root/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>True</code></pre>
</div>
</div>
<p>I have created a helper function in the next cell to preprocess and split the tweet text into tokens.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:38,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674282,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="im">import</span> re</span>
<span id="cb18-5"><a href="#cb18-5"></a></span>
<span id="cb18-6"><a href="#cb18-6"></a>sw <span class="op">=</span> stopwords.words(<span class="st">"english"</span>)</span>
<span id="cb18-7"><a href="#cb18-7"></a>wl <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb18-8"><a href="#cb18-8"></a></span>
<span id="cb18-9"><a href="#cb18-9"></a></span>
<span id="cb18-10"><a href="#cb18-10"></a><span class="kw">def</span> split_tokens(row):</span>
<span id="cb18-11"><a href="#cb18-11"></a>    <span class="co"># step 1: lower the text</span></span>
<span id="cb18-12"><a href="#cb18-12"></a>    t <span class="op">=</span> row[<span class="st">"tweet"</span>].lower()</span>
<span id="cb18-13"><a href="#cb18-13"></a>    <span class="co"># step 2: remove all other characters except alphabets, numbers, and a space</span></span>
<span id="cb18-14"><a href="#cb18-14"></a>    t <span class="op">=</span> re.sub(<span class="vs">r"[^a-z 0-9]"</span>, <span class="st">""</span>, t)</span>
<span id="cb18-15"><a href="#cb18-15"></a>    <span class="co"># step 3: split the text into words or tokens. split is made at each "space" character</span></span>
<span id="cb18-16"><a href="#cb18-16"></a>    t <span class="op">=</span> re.split(<span class="vs">r" +"</span>, t)</span>
<span id="cb18-17"><a href="#cb18-17"></a></span>
<span id="cb18-18"><a href="#cb18-18"></a>    <span class="co"># step 4: remove stop words</span></span>
<span id="cb18-19"><a href="#cb18-19"></a>    t <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> t <span class="cf">if</span> (i <span class="kw">not</span> <span class="kw">in</span> sw) <span class="kw">and</span> (i <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"user"</span>]) <span class="kw">and</span> <span class="bu">len</span>(i)]</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a>    <span class="co"># step 5: lemmatize words</span></span>
<span id="cb18-22"><a href="#cb18-22"></a>    t <span class="op">=</span> [wl.lemmatize(i) <span class="cf">for</span> i <span class="kw">in</span> t]</span>
<span id="cb18-23"><a href="#cb18-23"></a></span>
<span id="cb18-24"><a href="#cb18-24"></a>    row[<span class="st">"all_tokens"</span>] <span class="op">=</span> t</span>
<span id="cb18-25"><a href="#cb18-25"></a>    <span class="cf">return</span> row</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use a sample tweet text to uncover the working of this function.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:38,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674283,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="1a68394b-7cf6-479f-9730-0bd16f8a4629" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>sample_tweet <span class="op">=</span> train_ds[<span class="dv">0</span>][<span class="st">"tweet"</span>]</span>
<span id="cb19-2"><a href="#cb19-2"></a>sample_tweet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'</code></pre>
</div>
</div>
<p><strong>Step 1: lower the text</strong></p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:41,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674288,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="763b9635-593e-4cae-8dbc-9e608f00a843" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>t <span class="op">=</span> sample_tweet.lower()</span>
<span id="cb21-2"><a href="#cb21-2"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'</code></pre>
</div>
</div>
<p><strong>Step 2: remove all other characters except alphabets, numbers, and spaces</strong></p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:41,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674289,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="b4d25011-3464-4e4d-80d3-2af1befcb4da" data-execution_count="14">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>t <span class="op">=</span> re.sub(<span class="vs">r"[^a-z 0-9]"</span>, <span class="st">""</span>,t)</span>
<span id="cb23-2"><a href="#cb23-2"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>'user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run'</code></pre>
</div>
</div>
<p><strong>Step 3: split the text into words or tokens. split is made at each “space” character</strong></p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:40,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674290,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="681c3f73-3e91-45d3-e7b4-ceeb6e191a73" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>t <span class="op">=</span> re.split(<span class="vs">r" +"</span>, t)</span>
<span id="cb25-2"><a href="#cb25-2"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>['user',
 'when',
 'a',
 'father',
 'is',
 'dysfunctional',
 'and',
 'is',
 'so',
 'selfish',
 'he',
 'drags',
 'his',
 'kids',
 'into',
 'his',
 'dysfunction',
 'run']</code></pre>
</div>
</div>
<p><strong>Step 4: remove stop words</strong> Besides stop words, I have also filtered “user” from the text. This is because in the original tweet text, any reference to a Twitter user (e.g., <span class="citation" data-cites="hassaanbinaslam">@hassaanbinaslam</span>) is replaced with <code>@user</code> to hide identity.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:36,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674291,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="4f91ba1f-d70b-4360-b699-dd3ba027f787" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>t <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> t <span class="cf">if</span> (i <span class="kw">not</span> <span class="kw">in</span> sw) <span class="kw">and</span> (i <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"user"</span>]) <span class="kw">and</span> <span class="bu">len</span>(i)]</span>
<span id="cb27-2"><a href="#cb27-2"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']</code></pre>
</div>
</div>
<p>English language stop words taken from the NLTK library include the following list.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:33,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327674293,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="02e41b35-c74f-484e-83e8-a6f3f7270eca" data-execution_count="17">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># englist language stopwords from NLTK</span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="bu">print</span>(sw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</code></pre>
</div>
</div>
<p><strong>Step 5: lemmatize words</strong> <em>What is lemmatization anyway?</em> Lemmatization is applied to normalize the text. It considers the context and converts the word to its meaningful base form, which is called Lemma. to read more about it, refer to <a href="https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming">what-is-the-difference-between-lemmatization-vs-stemming</a></p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1659,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675924,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="ace851b5-5a4b-4721-f2da-f42d8d71b355" data-execution_count="18">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>t <span class="op">=</span> [wl.lemmatize(i) <span class="cf">for</span> i <span class="kw">in</span> t]</span>
<span id="cb31-2"><a href="#cb31-2"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>['father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']</code></pre>
</div>
</div>
<p>Let’s use the helper function <code>split_tokens</code> and compare our output.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:56,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675924,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="25c4e1d8-1fe8-4930-9491-f6655483461c" data-execution_count="19">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>split_tokens(train_ds[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>{'label': 0,
 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',
 'all_tokens': ['father',
  'dysfunctional',
  'selfish',
  'drag',
  'kid',
  'dysfunction',
  'run']}</code></pre>
</div>
</div>
<p>Our preprocessing steps and tokenize function is ready. So let’s apply it to our entire tweet dataset.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:50,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675925,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="2aaedbd6-ef73-4299-d773-383a22447258" data-execution_count="20">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># tokenize tweet dataset</span></span>
<span id="cb35-2"><a href="#cb35-2"></a>train_ds_all_tokens <span class="op">=</span> train_ds.<span class="bu">map</span>(split_tokens)</span>
<span id="cb35-3"><a href="#cb35-3"></a>train_ds_all_tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-4f6808d01b1a8972.arrow</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>Dataset({
    features: ['label', 'tweet', 'all_tokens'],
    num_rows: 31962
})</code></pre>
</div>
</div>
</section>
<section id="create-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="create-vocabulary">Create vocabulary</h3>
<p>A vocabulary is a dictionary where each <code>key</code> represents the token (word ) string form. The <code>value</code> obtained from the dictionary defines the token’s unique integer or encoded form. To create our vocabulary, we will follow the following approach.</p>
<ol type="1">
<li>Iterate through all the tweets (entire dataset) and count the frequency of all the tokens.</li>
<li>Remove the rare or uncommon tokens. This helps in reducing the size of the vocabulary. However, these uncommon tokens also include words created with typos e.g., while tweeting someone incorrectly typed <code>fahtr</code> instead of <code>father</code>. In our case, we will remove all tokens with a frequency of less than 10.</li>
<li>After that, we will put our filtered tokens in an ordered dictionary</li>
<li>Pass the ordered dictionary to Pytorch <a href="https://pytorch.org/text/stable/vocab.html">Vocab class</a>. A vocab object automatically maps tokens to indices.</li>
</ol>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:48,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675928,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="d0a494d4-c7ea-4af6-eccf-e9321638101b" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># create a frequency map for all toekns</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb38-3"><a href="#cb38-3"></a></span>
<span id="cb38-4"><a href="#cb38-4"></a>token_count <span class="op">=</span> Counter()</span>
<span id="cb38-5"><a href="#cb38-5"></a><span class="cf">for</span> row_tokens <span class="kw">in</span> train_ds_all_tokens[<span class="st">"all_tokens"</span>]:</span>
<span id="cb38-6"><a href="#cb38-6"></a>    token_count.update(row_tokens)</span>
<span id="cb38-7"><a href="#cb38-7"></a></span>
<span id="cb38-8"><a href="#cb38-8"></a><span class="bu">print</span>(<span class="st">"Number of tokens found: "</span>, <span class="bu">len</span>(token_count))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of tokens found:  38988</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:44,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675929,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="c4c93c5c-353f-4301-9343-377750dc58fb" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># remove uncommon tokens</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>min_token_freq <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb40-3"><a href="#cb40-3"></a>token_count_filtered <span class="op">=</span> {k: v <span class="cf">for</span> k, v <span class="kw">in</span> token_count.items() <span class="cf">if</span> v <span class="op">&gt;</span> min_token_freq}</span>
<span id="cb40-4"><a href="#cb40-4"></a></span>
<span id="cb40-5"><a href="#cb40-5"></a><span class="bu">print</span>(<span class="st">"Number of tokens after filtering: "</span>, <span class="bu">len</span>(token_count_filtered))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of tokens after filtering:  3007</code></pre>
</div>
</div>
<p>Notice that by removing uncommon tokens, we have significantly reduced the size of our vocabulary (almost 13x less). In the next step, we will sort them, convert them to <a href="https://realpython.com/python-ordereddict/">OrdreredDict</a>, and pass them to <code>torchtext.vocab</code>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:41,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675929,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="23">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> vocab</span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb42-3"><a href="#cb42-3"></a></span>
<span id="cb42-4"><a href="#cb42-4"></a><span class="co"># sort the tokens based on their frequency</span></span>
<span id="cb42-5"><a href="#cb42-5"></a>sorted_by_freq_tuples <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb42-6"><a href="#cb42-6"></a>    token_count_filtered.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb42-7"><a href="#cb42-7"></a>)</span>
<span id="cb42-8"><a href="#cb42-8"></a><span class="co"># create a dictionary of tokens</span></span>
<span id="cb42-9"><a href="#cb42-9"></a>ordered_dict <span class="op">=</span> OrderedDict(sorted_by_freq_tuples)</span>
<span id="cb42-10"><a href="#cb42-10"></a><span class="co"># convert the dictionary into a vocabulary</span></span>
<span id="cb42-11"><a href="#cb42-11"></a>vb <span class="op">=</span> vocab(ordered_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the following methods, we can use the vocabulary to move between the token’s integer and string form.</p>
<ul>
<li><code>vb['love']</code> to get the id ‘0’ for token ‘love’</li>
<li><code>vb.get_stoi()['love']</code> to get id ‘0’ for token ‘love’</li>
<li><code>vb.get_itos()[0]</code> to get token ‘love’ from id ‘0’</li>
</ul>
<p>vb.lookup_token(0) vb.lookup_tokens([0]) vb.lookup_indices([0])</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:40,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675929,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="2e083e6d-5d6c-4e39-8e15-27b07a7570ac" data-execution_count="24">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>vb[<span class="st">"love"</span>], vb.get_stoi()[<span class="st">"love"</span>], vb.get_itos()[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>(0, 0, 'love')</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:38,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675930,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="5f7097ef-d395-403e-c9fd-c85b4a0b16b8" data-execution_count="25">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="co"># let's check a few more tokens</span></span>
<span id="cb45-2"><a href="#cb45-2"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb45-3"><a href="#cb45-3"></a>    token <span class="op">=</span> vb.get_itos()[i]</span>
<span id="cb45-4"><a href="#cb45-4"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> ---&gt; </span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb45-5"><a href="#cb45-5"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss"> ---&gt; </span><span class="sc">{</span>vb<span class="sc">.</span>get_stoi()[token]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb45-6"><a href="#cb45-6"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 ---&gt; love
love ---&gt; 0

1 ---&gt; day
day ---&gt; 1

2 ---&gt; happy
happy ---&gt; 2

3 ---&gt; u
u ---&gt; 3

4 ---&gt; amp
amp ---&gt; 4
</code></pre>
</div>
</div>
<p>Alright, we have our vocabulary ready. Remember that we have filtered our vocabulary to remove uncommon tokens. Let’s use this vocabulary to do the same for our tweet text tokens.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:32,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675930,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="4a263a03-04bf-4554-db72-1406f3ec4d9e" data-execution_count="26">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a><span class="co"># use vocabulary to filter uncommon tokens from tweets</span></span>
<span id="cb47-2"><a href="#cb47-2"></a><span class="kw">def</span> remove_rare_tokens(row):</span>
<span id="cb47-3"><a href="#cb47-3"></a>    row[<span class="st">"tokens"</span>] <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> row[<span class="st">"all_tokens"</span>] <span class="cf">if</span> t <span class="kw">in</span> vb]</span>
<span id="cb47-4"><a href="#cb47-4"></a>    <span class="cf">return</span> row</span>
<span id="cb47-5"><a href="#cb47-5"></a></span>
<span id="cb47-6"><a href="#cb47-6"></a></span>
<span id="cb47-7"><a href="#cb47-7"></a>train_ds_tokens <span class="op">=</span> train_ds_all_tokens.<span class="bu">map</span>(remove_rare_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-84d95f24a0e6d681.arrow</code></pre>
</div>
</div>
<p>In the following cell output, note that only three tokens are left for a sample tweet after removing uncommon tokens.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675931,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="8bcc2aea-2080-4533-b30d-f136e8c1495f" data-execution_count="27">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="co"># verify the dataset after filtering</span></span>
<span id="cb49-2"><a href="#cb49-2"></a>train_ds_tokens[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>{'label': 0,
 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',
 'all_tokens': ['father',
  'dysfunctional',
  'selfish',
  'drag',
  'kid',
  'dysfunction',
  'run'],
 'tokens': ['father', 'kid', 'run']}</code></pre>
</div>
</div>
</section>
<section id="create-training-samples-using-skip-gram" class="level3">
<h3 class="anchored" data-anchor-id="create-training-samples-using-skip-gram">Create training samples using skip-gram</h3>
<p>The next step is to create training samples using a ” skip-gram ” technique. To understand it, let’s take an example of a sample tweet text: <code>i get to see my daddy today!</code>. After applying preprocessing steps, the word tokens produced from the tweet are: <code>['get', 'see', 'daddy', 'today']</code>. The vocabulary indices (integer value) for these tokens are provided below.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Tokens</strong></th>
<th style="text-align: center;"><strong>Indices</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">get</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">see</td>
<td style="text-align: center;">22</td>
</tr>
<tr class="odd">
<td style="text-align: center;">daddy</td>
<td style="text-align: center;">404</td>
</tr>
<tr class="even">
<td style="text-align: center;">today</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<p>For creating training samples using skip-gram, we take each token as an input and a surrounding token as the label. But we also need to decide the window size, meaning how many surrounding tokens to create training samples. For example, suppose we take <code>window_size=2</code>; then the training sample for this tweet will be.</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>text</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>input</th>
<th>output</th>
<th></th>
<th>input</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>get</strong></td>
<td>see</td>
<td>daddy</td>
<td>today</td>
<td>—&gt;</td>
<td>get</td>
<td>see</td>
<td>—&gt;</td>
<td>10</td>
<td>22</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>get</td>
<td>daddy</td>
<td></td>
<td>10</td>
<td>404</td>
</tr>
<tr class="odd">
<td>get</td>
<td><strong>see</strong></td>
<td>daddy</td>
<td>today</td>
<td>—&gt;</td>
<td>see</td>
<td>get</td>
<td>—&gt;</td>
<td>22</td>
<td>10</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>see</td>
<td>daddy</td>
<td></td>
<td>22</td>
<td>404</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>see</td>
<td>today</td>
<td></td>
<td>22</td>
<td>9</td>
</tr>
<tr class="even">
<td>get</td>
<td>see</td>
<td><strong>daddy</strong></td>
<td>today</td>
<td>—&gt;</td>
<td>daddy</td>
<td>get</td>
<td>—&gt;</td>
<td>404</td>
<td>10</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>daddy</td>
<td>see</td>
<td></td>
<td>404</td>
<td>22</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>daddy</td>
<td>today</td>
<td></td>
<td>404</td>
<td>9</td>
</tr>
<tr class="odd">
<td>get</td>
<td>see</td>
<td>daddy</td>
<td><strong>today</strong></td>
<td>—&gt;</td>
<td>today</td>
<td>see</td>
<td>—&gt;</td>
<td>9</td>
<td>22</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>today</td>
<td>daddy</td>
<td></td>
<td>9</td>
<td>404</td>
</tr>
</tbody>
</table>
<p>In the above table, we have generated ten training samples from 4 tokens with a window size of 2. In each sample, we have two tokens</p>
<ul>
<li>input or the training data</li>
<li>output or the label value</li>
</ul>
<p>Given the tokens and window size, let’s create a function to create training samples for us.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:26,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675931,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="e2dcd800-ce92-4da4-8aa7-f1115a165f62" data-execution_count="28">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a><span class="co"># code source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html</span></span>
<span id="cb51-2"><a href="#cb51-2"></a><span class="kw">def</span> windowizer(row, wsize<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb51-3"><a href="#cb51-3"></a>    <span class="co"># default 'window size' (wsize) is three</span></span>
<span id="cb51-4"><a href="#cb51-4"></a>    doc <span class="op">=</span> row[<span class="st">"tokens"</span>]</span>
<span id="cb51-5"><a href="#cb51-5"></a>    wsize <span class="op">=</span> wsize</span>
<span id="cb51-6"><a href="#cb51-6"></a>    out <span class="op">=</span> []</span>
<span id="cb51-7"><a href="#cb51-7"></a>    <span class="cf">for</span> i, wd <span class="kw">in</span> <span class="bu">enumerate</span>(doc):</span>
<span id="cb51-8"><a href="#cb51-8"></a>        target <span class="op">=</span> vb[wd]</span>
<span id="cb51-9"><a href="#cb51-9"></a>        window <span class="op">=</span> [</span>
<span id="cb51-10"><a href="#cb51-10"></a>            i <span class="op">+</span> j</span>
<span id="cb51-11"><a href="#cb51-11"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="op">-</span>wsize, wsize <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb51-12"><a href="#cb51-12"></a>            <span class="cf">if</span> (i <span class="op">+</span> j <span class="op">&gt;=</span> <span class="dv">0</span>) <span class="op">&amp;</span> (i <span class="op">+</span> j <span class="op">&lt;</span> <span class="bu">len</span>(doc)) <span class="op">&amp;</span> (j <span class="op">!=</span> <span class="dv">0</span>)</span>
<span id="cb51-13"><a href="#cb51-13"></a>        ]</span>
<span id="cb51-14"><a href="#cb51-14"></a></span>
<span id="cb51-15"><a href="#cb51-15"></a>        out <span class="op">+=</span> [(target, vb[doc[w]]) <span class="cf">for</span> w <span class="kw">in</span> window]</span>
<span id="cb51-16"><a href="#cb51-16"></a>    row[<span class="st">"moving_window"</span>] <span class="op">=</span> out</span>
<span id="cb51-17"><a href="#cb51-17"></a>    <span class="cf">return</span> row</span>
<span id="cb51-18"><a href="#cb51-18"></a></span>
<span id="cb51-19"><a href="#cb51-19"></a></span>
<span id="cb51-20"><a href="#cb51-20"></a>train_ds_tokens <span class="op">=</span> train_ds_tokens.<span class="bu">map</span>(windowizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-e1806b88a0961df3.arrow</code></pre>
</div>
</div>
<p>Let’s check a sample tweet with generated training samples.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:23,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675931,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="2dd79388-84f6-48f0-a044-1ce98da4a104" data-execution_count="29">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a><span class="co"># note that we have used skip-gram window size=3</span></span>
<span id="cb53-2"><a href="#cb53-2"></a>train_ds_tokens[<span class="dv">12</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>{'label': 0,
 'tweet': 'i get to see my daddy today!!   #80days #gettingfed',
 'all_tokens': ['get', 'see', 'daddy', 'today', '80days', 'gettingfed'],
 'tokens': ['get', 'see', 'daddy', 'today'],
 'moving_window': [[10, 22],
  [10, 404],
  [10, 9],
  [22, 10],
  [22, 404],
  [22, 9],
  [404, 10],
  [404, 22],
  [404, 9],
  [9, 10],
  [9, 22],
  [9, 404]]}</code></pre>
</div>
</div>
</section>
<section id="create-dataset-and-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="create-dataset-and-dataloader">Create dataset and dataloader</h3>
<p>The preprocessing part of the data is complete. Now we only need to load this data into the Pytorch Dataset class and create batches for model training using DataLoader class. Both are utilities or helper classes provided by PyTorch to make our dataset (data preparation) code decoupled from our model training code for better readability and modularity.</p>
<ul>
<li><code>torch.utils.data.Dataset</code> stores the samples and their corresponding labels</li>
<li><code>torch.utils.data.DataLoader</code> wraps an iterable around the Dataset to enable easy access to the samples.</li>
</ul>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:18,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327675932,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="30">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a><span class="co"># source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html</span></span>
<span id="cb55-2"><a href="#cb55-2"></a><span class="co"># Helper class to make our data compatible with PyTorch Dataset</span></span>
<span id="cb55-3"><a href="#cb55-3"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb55-4"><a href="#cb55-4"></a></span>
<span id="cb55-5"><a href="#cb55-5"></a><span class="kw">class</span> Word2VecDataset(Dataset):</span>
<span id="cb55-6"><a href="#cb55-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dataset, vocab_size):</span>
<span id="cb55-7"><a href="#cb55-7"></a>        <span class="va">self</span>.dataset <span class="op">=</span> dataset</span>
<span id="cb55-8"><a href="#cb55-8"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb55-9"><a href="#cb55-9"></a>        <span class="va">self</span>.data <span class="op">=</span> [i <span class="cf">for</span> s <span class="kw">in</span> dataset[<span class="st">"moving_window"</span>] <span class="cf">for</span> i <span class="kw">in</span> s]</span>
<span id="cb55-10"><a href="#cb55-10"></a></span>
<span id="cb55-11"><a href="#cb55-11"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb55-12"><a href="#cb55-12"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb55-13"><a href="#cb55-13"></a></span>
<span id="cb55-14"><a href="#cb55-14"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb55-15"><a href="#cb55-15"></a>        <span class="cf">return</span> <span class="va">self</span>.data[idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3554,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327679470,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="6ae75b90-0a0d-4e0e-b1f6-75516c33ad86" data-execution_count="31">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a>vb_size <span class="op">=</span> <span class="bu">len</span>(vb)</span>
<span id="cb56-2"><a href="#cb56-2"></a></span>
<span id="cb56-3"><a href="#cb56-3"></a>word2vec_ds <span class="op">=</span> Word2VecDataset(train_ds_tokens, vocab_size<span class="op">=</span>vb_size)</span>
<span id="cb56-4"><a href="#cb56-4"></a></span>
<span id="cb56-5"><a href="#cb56-5"></a><span class="bu">print</span>(</span>
<span id="cb56-6"><a href="#cb56-6"></a>    <span class="ss">f"a word2vec_ds entry: </span><span class="sc">{</span>word2vec_ds[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb56-7"><a href="#cb56-7"></a>    <span class="ss">f"</span><span class="ch">\n</span><span class="ss">number of word2vec_ds entries: </span><span class="sc">{</span><span class="bu">len</span>(word2vec_ds)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb56-8"><a href="#cb56-8"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>a word2vec_ds entry: [13, 123] 
number of word2vec_ds entries: 761044</code></pre>
</div>
</div>
<p><code>word2vec_ds</code> is our training dataset, and a single entry from it is of shape <code>(input, label)</code>. Notice that there are many entries (or training samples) in <code>word2vec_ds</code>, and these samples represent all of the tweets tokens. For training, we need to create batches from them for faster processing. So let’s do that next.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:9,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327679471,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="60244f48-959c-4a7e-9cb5-dfdf8315107a" data-execution_count="32">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb58-2"><a href="#cb58-2"></a></span>
<span id="cb58-3"><a href="#cb58-3"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">14</span></span>
<span id="cb58-4"><a href="#cb58-4"></a>N_LOADER_PROCS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb58-5"><a href="#cb58-5"></a></span>
<span id="cb58-6"><a href="#cb58-6"></a>dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb58-7"><a href="#cb58-7"></a>    word2vec_ds, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span>N_LOADER_PROCS</span>
<span id="cb58-8"><a href="#cb58-8"></a>)</span>
<span id="cb58-9"><a href="#cb58-9"></a></span>
<span id="cb58-10"><a href="#cb58-10"></a><span class="bu">print</span>(<span class="ss">f"number of training batches: </span><span class="sc">{</span><span class="bu">len</span>(dataloader)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>number of training batches: 47</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2187,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327681651,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="fda7bb7f-553c-4d24-ae83-992e9730e252" data-execution_count="33">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>tmp <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb61-2"><a href="#cb61-2"></a>tmp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>[tensor([  46, 1743,    0,  ...,   28,    8,  122]),
 tensor([  15, 2197,  197,  ...,  179,  846,    0])]</code></pre>
</div>
</div>
<p>In the last cell, I fetched a single batch from the dataloader. Its output is two long tensors.</p>
<ul>
<li>The first tensor has all the entries for the input data</li>
<li>The second tensor has all the entries for the labels</li>
</ul>
<p>The size of both these tensors is the same.</p>
</section>
</section>
<section id="model-configuration" class="level2">
<h2 class="anchored" data-anchor-id="model-configuration">Model Configuration</h2>
<p>Now we will configure the model to be used for training.</p>
<p><strong>What are we trying to solve with our model?</strong></p>
<p>We are trying to train a model that can predict the surrounding words of a given input. And this is how we have designed our training data too. Each training sample row has <code>(input, label)</code>. Where <code>input</code> is the given token, and <code>label</code> is some nearby token. It is like forcing the model to predict the context of an input word.</p>
<p><strong>How can such a model be helpful to us?</strong></p>
<p>Such a model that can predict the context of a word is not helpful to us in any actual scenario. So we are not going to use it. Instead, we are only interested in the learned weights of such a model, and we call them <code>word embedding</code>. It is like faking a problem (creating a pseudo-problem), training a model, and then using the learned weights for other tasks.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:11,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327681652,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="34">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a><span class="co"># source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html</span></span>
<span id="cb63-2"><a href="#cb63-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb63-3"><a href="#cb63-3"></a></span>
<span id="cb63-4"><a href="#cb63-4"></a><span class="kw">class</span> Word2Vec(nn.Module):</span>
<span id="cb63-5"><a href="#cb63-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_size):</span>
<span id="cb63-6"><a href="#cb63-6"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-7"><a href="#cb63-7"></a>        <span class="va">self</span>.embed <span class="op">=</span> nn.Embedding(vocab_size, embedding_size)</span>
<span id="cb63-8"><a href="#cb63-8"></a>        <span class="va">self</span>.expand <span class="op">=</span> nn.Linear(embedding_size, vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-9"><a href="#cb63-9"></a></span>
<span id="cb63-10"><a href="#cb63-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb63-11"><a href="#cb63-11"></a>        <span class="co"># Encode input to lower-dimensional representation</span></span>
<span id="cb63-12"><a href="#cb63-12"></a>        hidden <span class="op">=</span> <span class="va">self</span>.embed(<span class="bu">input</span>)</span>
<span id="cb63-13"><a href="#cb63-13"></a>        <span class="co"># Expand hidden layer to predictions</span></span>
<span id="cb63-14"><a href="#cb63-14"></a>        logits <span class="op">=</span> <span class="va">self</span>.expand(hidden)</span>
<span id="cb63-15"><a href="#cb63-15"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that in the model configuration, the size of the output layer (nn.Linear) is equal to our vocabulary size. So, our model is configured for a multi-classification problem.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3241,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327684884,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="31a39ef4-fa97-4536-9183-c0c2e137138e" data-execution_count="35">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>EMBED_SIZE <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb64-2"><a href="#cb64-2"></a>model <span class="op">=</span> Word2Vec(vb_size, EMBED_SIZE)</span>
<span id="cb64-3"><a href="#cb64-3"></a></span>
<span id="cb64-4"><a href="#cb64-4"></a><span class="co"># Relevant if you have a GPU</span></span>
<span id="cb64-5"><a href="#cb64-5"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb64-6"><a href="#cb64-6"></a><span class="bu">print</span>(device)</span>
<span id="cb64-7"><a href="#cb64-7"></a></span>
<span id="cb64-8"><a href="#cb64-8"></a>model.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cuda</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>Word2Vec(
  (embed): Embedding(3007, 100)
  (expand): Linear(in_features=100, out_features=3007, bias=False)
)</code></pre>
</div>
</div>
<p>Now configure the loss function and a training optimizer.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:15,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670327684885,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="36">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a><span class="im">import</span> torch</span>
<span id="cb67-2"><a href="#cb67-2"></a></span>
<span id="cb67-3"><a href="#cb67-3"></a>LR <span class="op">=</span> <span class="fl">3e-4</span>  <span class="co"># learning rate</span></span>
<span id="cb67-4"><a href="#cb67-4"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb67-5"><a href="#cb67-5"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>LR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-training" class="level2">
<h2 class="anchored" data-anchor-id="model-training">Model Training</h2>
<p>In this section we are going to train our model for 100 epochs.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:359323,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044195,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="d08cc511-af3d-4865-cfac-970136f99685" data-execution_count="37">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a><span class="co"># source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html</span></span>
<span id="cb68-2"><a href="#cb68-2"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm  <span class="co"># For progress bar. https://github.com/tqdm/tqdm</span></span>
<span id="cb68-3"><a href="#cb68-3"></a></span>
<span id="cb68-4"><a href="#cb68-4"></a>EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb68-5"><a href="#cb68-5"></a>progress_bar <span class="op">=</span> tqdm(<span class="bu">range</span>(EPOCHS <span class="op">*</span> <span class="bu">len</span>(dataloader)))</span>
<span id="cb68-6"><a href="#cb68-6"></a>running_loss <span class="op">=</span> []</span>
<span id="cb68-7"><a href="#cb68-7"></a></span>
<span id="cb68-8"><a href="#cb68-8"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb68-9"><a href="#cb68-9"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb68-10"><a href="#cb68-10"></a>    <span class="cf">for</span> center, context <span class="kw">in</span> dataloader:</span>
<span id="cb68-11"><a href="#cb68-11"></a>        center, context <span class="op">=</span> center.to(device), context.to(device)</span>
<span id="cb68-12"><a href="#cb68-12"></a></span>
<span id="cb68-13"><a href="#cb68-13"></a>        optimizer.zero_grad()</span>
<span id="cb68-14"><a href="#cb68-14"></a>        logits <span class="op">=</span> model(<span class="bu">input</span><span class="op">=</span>context)</span>
<span id="cb68-15"><a href="#cb68-15"></a>        loss <span class="op">=</span> loss_fn(logits, center)</span>
<span id="cb68-16"><a href="#cb68-16"></a>        epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb68-17"><a href="#cb68-17"></a>        loss.backward()</span>
<span id="cb68-18"><a href="#cb68-18"></a>        optimizer.step()</span>
<span id="cb68-19"><a href="#cb68-19"></a></span>
<span id="cb68-20"><a href="#cb68-20"></a>        progress_bar.update(<span class="dv">1</span>)</span>
<span id="cb68-21"><a href="#cb68-21"></a>    epoch_loss <span class="op">/=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb68-22"><a href="#cb68-22"></a>    running_loss.append(epoch_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|█████████▉| 4699/4700 [05:59&lt;00:00, 40.21it/s]</code></pre>
</div>
</div>
<p>Let’s plot the training loss.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:50,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044200,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="6f41f22b-b0bc-4aa4-db2b-38a71670ceeb" data-execution_count="38">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb70-2"><a href="#cb70-2"></a>plt.plot(running_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-12-02-pytorch-word2vec-embedding_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="analyze-word-embeddings-word-vectors" class="level2">
<h2 class="anchored" data-anchor-id="analyze-word-embeddings-word-vectors">Analyze Word Embeddings (Word Vectors)</h2>
<p>Training is complete, and our model has learned something. In this section, we will analyze the learned weights and their quality.</p>
<p>So, let’s extract the weights.</p>
<p><strong>We have weights in two layers: embedding and linear. Both have similar dimensions, so which should we use as embeddings?</strong> I have experimented with weights from both these layers, and here are some suggestions.</p>
<ul>
<li>Embeddings from the layer closer to the output layer give better results. In our case, it is the Linear layer. Our linear layer has the same dimensions as the embedding layer, so that we will use weights from this layer.</li>
<li>If the dimensions of the layers closer to the output layer are different than the Embedding layer, then we should stick to the embedding layer. Sometimes, people prefer to concatenate weights from multiple layers to create the final output.</li>
</ul>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:40,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044200,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="9549b15f-6def-4198-e198-19ed217c0e78" data-execution_count="39">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a><span class="co"># embedding = model.embed.weight.cpu().detach().numpy()</span></span>
<span id="cb71-2"><a href="#cb71-2"></a>embedding <span class="op">=</span> model.expand.weight.cpu().detach().numpy()</span>
<span id="cb71-3"><a href="#cb71-3"></a>embedding.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(3007, 100)</code></pre>
</div>
</div>
<p>Next, I have created some functions to calculate the distance between these word vectors. We are doing this to find a word’s (cosine) similarity score with other words. A smaller distance between two vectors means that these words are often used in similar contexts.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:34,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044201,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="40">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1"></a><span class="co"># source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html</span></span>
<span id="cb73-2"><a href="#cb73-2"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance</span>
<span id="cb73-3"><a href="#cb73-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb73-4"><a href="#cb73-4"></a></span>
<span id="cb73-5"><a href="#cb73-5"></a><span class="kw">def</span> get_distance_matrix(embedding, metric):</span>
<span id="cb73-6"><a href="#cb73-6"></a>    dist_matrix <span class="op">=</span> distance.squareform(distance.pdist(embedding, metric))</span>
<span id="cb73-7"><a href="#cb73-7"></a>    <span class="cf">return</span> dist_matrix</span>
<span id="cb73-8"><a href="#cb73-8"></a></span>
<span id="cb73-9"><a href="#cb73-9"></a></span>
<span id="cb73-10"><a href="#cb73-10"></a><span class="kw">def</span> get_k_similar_words(word, dist_matrix, k<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb73-11"><a href="#cb73-11"></a>    idx <span class="op">=</span> vb[word]</span>
<span id="cb73-12"><a href="#cb73-12"></a>    dists <span class="op">=</span> dist_matrix[idx]</span>
<span id="cb73-13"><a href="#cb73-13"></a>    ind <span class="op">=</span> np.argpartition(dists, k)[: k <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb73-14"><a href="#cb73-14"></a>    ind <span class="op">=</span> ind[np.argsort(dists[ind])][<span class="dv">1</span>:]</span>
<span id="cb73-15"><a href="#cb73-15"></a>    out <span class="op">=</span> [(i, vb.lookup_token(i), dists[i]) <span class="cf">for</span> i <span class="kw">in</span> ind]</span>
<span id="cb73-16"><a href="#cb73-16"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:805,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044973,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="1c7723ae-ade8-4503-c76d-fd913cc2423e" data-execution_count="41">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1"></a><span class="co"># calculate 2d distance matrix</span></span>
<span id="cb74-2"><a href="#cb74-2"></a>dmat <span class="op">=</span> get_distance_matrix(embedding, <span class="st">"cosine"</span>)</span>
<span id="cb74-3"><a href="#cb74-3"></a>dmat.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>(3007, 3007)</code></pre>
</div>
</div>
<p>Another helper function is to print similar words identified based on their distance.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:22,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044974,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="42">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1"></a><span class="kw">def</span> similar_words(tokens):</span>
<span id="cb76-2"><a href="#cb76-2"></a>    <span class="cf">for</span> word <span class="kw">in</span> tokens:</span>
<span id="cb76-3"><a href="#cb76-3"></a>        <span class="bu">print</span>(word, [t[<span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> get_k_similar_words(word, dmat)], <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:23,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044975,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="b5d256af-d190-4645-b458-3ddb3bb4e32e" data-execution_count="43">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1"></a>tokens <span class="op">=</span> [<span class="st">"father"</span>, <span class="st">"mother"</span>, <span class="st">"boy"</span>, <span class="st">"girl"</span>]  <span class="co"># tokens for relations</span></span>
<span id="cb77-2"><a href="#cb77-2"></a>tokens <span class="op">+=</span> [<span class="st">"job"</span>, <span class="st">"sad"</span>, <span class="st">"happy"</span>, <span class="st">"hate"</span>]  <span class="co"># for emotions</span></span>
<span id="cb77-3"><a href="#cb77-3"></a>tokens <span class="op">+=</span> [<span class="st">"america"</span>, <span class="st">"england"</span>, <span class="st">"india"</span>]  <span class="co"># for regions</span></span>
<span id="cb77-4"><a href="#cb77-4"></a>tokens <span class="op">+=</span> [<span class="st">"football"</span>, <span class="st">"swimming"</span>, <span class="st">"cycling"</span>]  <span class="co"># for sports</span></span>
<span id="cb77-5"><a href="#cb77-5"></a>tokens <span class="op">+=</span> [<span class="st">"exercise"</span>, <span class="st">"health"</span>, <span class="st">"fitness"</span>]  <span class="co"># for health</span></span>
<span id="cb77-6"><a href="#cb77-6"></a></span>
<span id="cb77-7"><a href="#cb77-7"></a>similar_words(tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>father ['dad', 'fathersday', 'day', 'happy', 'love', 'one', 'bihday', 'kid', 'u', 'today'] 

mother ['care', 'child', 'son', 'died', 'father', 'kill', 'men', 'ten', 'born', 'wife'] 

boy ['girl', 'guy', 'man', 'smile', 'little', 'family', 'love', 'selfie', 'day', 'face'] 

girl ['smile', 'summer', 'love', 'guy', 'happy', 'fun', 'friend', 'boy', 'beautiful', 'today'] 

job ['one', 'amp', 'im', 'getting', 'got', 'going', 'first', 'even', 'get', 'cant'] 

sad ['make', 'people', 'life', 'know', 'dont', 'help', 'world', 'amp', 'like', 'say'] 

happy ['day', 'today', 'love', 'great', 'life', 'make', 'amp', 'weekend', 'smile', 'one'] 

hate ['people', 'america', 'say', 'dont', 'even', 'many', 'still', 'world', 'much', 'amp'] 

america ['hate', 'orlando', 'many', 'trump', 'people', 'say', 'even', 'american', 'still', 'shooting'] 

england ['eng', 'football', 'euro2016', 'v', 'wale', 'soccer', 'match', 'russia', 'player', 'clinton'] 

india ['received', 'test', 'sign', 'local', 'em', 'rude', 'amount', 'forget', 'called', 'shocking'] 

football ['england', 'euro2016', 'france', 'fan', 'v', 'making', 'review', 'match', 'game', 'award'] 

swimming ['swim', 'bbq', 'sunglass', 'ink', 'lovemylife', 'ceremony', 'placement', 'follow4follow', 'brunette', 'pool'] 

cycling ['pub', 'nutrition', 'musictherapy', 'exploring', 'letsgo', 'niece', 'cook', 'taste', 'pougal', 'mount'] 

exercise ['wellness', 'weightloss', 'lifeisgood', 'fitfam', 'inspire', 'madrid', 'namaste', 'runner', 'yoga', 'workout'] 

health ['healthy', 'food', 'happiness', 'amazing', 'fitness', 'beauty', 'gym', 'positive', 'run', 'lifestyle'] 

fitness ['workout', 'gym', 'fit', 'food', 'running', 'run', 'yoga', 'health', 'selfie', 'monday'] 
</code></pre>
</div>
</div>
<section id="visualize-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="visualize-embeddings">Visualize embeddings</h3>
<p>We can also analyze the embeddings by visualizing them on a plot. Similar word vectors should appear closer to the plot. But each word vector is dimension 300, so how can we plot them on a 2D surface? For this, we can take the help of the dimension reduction technique t-SNE to reduce the word vectors dimensions from 300 to 2. PCA can also be applied to achieve similar results.</p>
<p>We need first to normalize the weights to get better results.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:19,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328044976,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="cd5aa0b1-6b13-4c65-c482-564880993dde" data-execution_count="44">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1"></a><span class="co"># source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb</span></span>
<span id="cb79-2"><a href="#cb79-2"></a><span class="co"># normalization of weights</span></span>
<span id="cb79-3"><a href="#cb79-3"></a>norms <span class="op">=</span> (embedding <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>) <span class="op">**</span> (<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb79-4"><a href="#cb79-4"></a>norms <span class="op">=</span> np.reshape(norms, (<span class="bu">len</span>(norms), <span class="dv">1</span>))</span>
<span id="cb79-5"><a href="#cb79-5"></a>embedding_norm <span class="op">=</span> embedding <span class="op">/</span> norms</span>
<span id="cb79-6"><a href="#cb79-6"></a></span>
<span id="cb79-7"><a href="#cb79-7"></a>embedding_norm.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>(3007, 100)</code></pre>
</div>
</div>
<p>Now apply t-distributed Stochastic Neighbor Embedding (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a>) to reduce the dimensions of our embedding.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:29761,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328074723,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="6c1a1cfa-45cf-4b87-fc22-62bb04cd1164" data-execution_count="45">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1"></a><span class="co"># source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb</span></span>
<span id="cb81-2"><a href="#cb81-2"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb81-3"><a href="#cb81-3"></a></span>
<span id="cb81-4"><a href="#cb81-4"></a><span class="co"># embeddings DataFrame</span></span>
<span id="cb81-5"><a href="#cb81-5"></a>embedding_df <span class="op">=</span> pd.DataFrame(embedding)</span>
<span id="cb81-6"><a href="#cb81-6"></a></span>
<span id="cb81-7"><a href="#cb81-7"></a><span class="co"># t-SNE transform</span></span>
<span id="cb81-8"><a href="#cb81-8"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb81-9"><a href="#cb81-9"></a>embedding_df_trans <span class="op">=</span> tsne.fit_transform(embedding_df)</span>
<span id="cb81-10"><a href="#cb81-10"></a>embedding_df_trans <span class="op">=</span> pd.DataFrame(embedding_df_trans)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
  warnings.warn(
100%|██████████| 4700/4700 [06:10&lt;00:00, 40.21it/s]</code></pre>
</div>
</div>
<p>We have some tokens selected in the last section. Let’s highlight them in the plot to locate them easily. For this, I have created a color coding for their indices.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:27,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328074724,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-execution_count="46">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1"></a><span class="co"># get token order</span></span>
<span id="cb83-2"><a href="#cb83-2"></a>embedding_df_trans.index <span class="op">=</span> vb.get_itos()</span>
<span id="cb83-3"><a href="#cb83-3"></a></span>
<span id="cb83-4"><a href="#cb83-4"></a><span class="co"># create color codes for selected tokens</span></span>
<span id="cb83-5"><a href="#cb83-5"></a>color_codes <span class="op">=</span> []</span>
<span id="cb83-6"><a href="#cb83-6"></a><span class="cf">for</span> s <span class="kw">in</span> embedding_df_trans.index:</span>
<span id="cb83-7"><a href="#cb83-7"></a>    <span class="cf">if</span> s <span class="kw">in</span> tokens:</span>
<span id="cb83-8"><a href="#cb83-8"></a>        color_codes.append(<span class="va">True</span>)</span>
<span id="cb83-9"><a href="#cb83-9"></a>    <span class="cf">else</span>:</span>
<span id="cb83-10"><a href="#cb83-10"></a>        color_codes.append(<span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now create a scatter plot of these embeddings using Plotly.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:28,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1670328074726,&quot;user&quot;:{&quot;displayName&quot;:&quot;Hassaan Bin Aslam&quot;,&quot;userId&quot;:&quot;13411116435427293553&quot;},&quot;user_tz&quot;:-300}" data-outputid="0577001b-6040-472f-c88d-6a4f364f2d70">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1"></a><span class="co"># source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb</span></span>
<span id="cb84-2"><a href="#cb84-2"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb84-3"><a href="#cb84-3"></a></span>
<span id="cb84-4"><a href="#cb84-4"></a>color <span class="op">=</span> np.where(color_codes, <span class="st">"red"</span>, <span class="st">"grey"</span>)</span>
<span id="cb84-5"><a href="#cb84-5"></a>fig <span class="op">=</span> go.Figure()</span>
<span id="cb84-6"><a href="#cb84-6"></a></span>
<span id="cb84-7"><a href="#cb84-7"></a>fig.add_trace(</span>
<span id="cb84-8"><a href="#cb84-8"></a>    go.Scatter(</span>
<span id="cb84-9"><a href="#cb84-9"></a>        x<span class="op">=</span>embedding_df_trans[<span class="dv">0</span>],</span>
<span id="cb84-10"><a href="#cb84-10"></a>        y<span class="op">=</span>embedding_df_trans[<span class="dv">1</span>],</span>
<span id="cb84-11"><a href="#cb84-11"></a>        mode<span class="op">=</span><span class="st">"text"</span>,</span>
<span id="cb84-12"><a href="#cb84-12"></a>        text<span class="op">=</span>embedding_df_trans.index,</span>
<span id="cb84-13"><a href="#cb84-13"></a>        textposition<span class="op">=</span><span class="st">"middle center"</span>,</span>
<span id="cb84-14"><a href="#cb84-14"></a>        textfont<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>color),</span>
<span id="cb84-15"><a href="#cb84-15"></a>    )</span>
<span id="cb84-16"><a href="#cb84-16"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="images/2022-12-02-pytorch-word2vec-embedding/plot-1.png" class="img-fluid"></p>
<p>I am zooming in on some of the highlighted tokens at the top right corner.</p>
<p><img src="images/2022-12-02-pytorch-word2vec-embedding/plot-2.png" class="img-fluid"></p>
<p>Further zooming.</p>
<p><img src="images/2022-12-02-pytorch-word2vec-embedding/plot-3.png" class="img-fluid"></p>


</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"0700ca7be4e24998a2a3205b7b9c7bf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e1dbf44aaa447e99f1c449bb5294e33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eddd7378b7047e3ad7a1216ba184918":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e433938116347a59ce144ef862af928":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"427f8f7d1a374f70915a8c10ae7ecc14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e1dbf44aaa447e99f1c449bb5294e33","placeholder":"​","style":"IPY_MODEL_3e433938116347a59ce144ef862af928","value":"100%"}},"4d4a8f416b2e405fbdc80d0fe6cce353":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eddd7378b7047e3ad7a1216ba184918","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e562176c3e0341a3a1bb881645c59029","value":1}},"97ed0d18e7a14227872981fc581f550f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a24de2b54c604b0784c52177cfbd6fd0","placeholder":"​","style":"IPY_MODEL_0700ca7be4e24998a2a3205b7b9c7bf0","value":" 1/1 [00:00&lt;00:00, 27.97it/s]"}},"a24de2b54c604b0784c52177cfbd6fd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00236fd7d5b4a0dac70c4389d20e61d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e562176c3e0341a3a1bb881645c59029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd43128fc4d74e10a6d4d64df3d3195a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_427f8f7d1a374f70915a8c10ae7ecc14","IPY_MODEL_4d4a8f416b2e405fbdc80d0fe6cce353","IPY_MODEL_97ed0d18e7a14227872981fc581f550f"],"layout":"IPY_MODEL_d00236fd7d5b4a0dac70c4389d20e61d"}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="hassaanbinaslam/myblog_utterances" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>